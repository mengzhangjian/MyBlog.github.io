<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alpha</title>
    <description>Every failure is leading towards success.</description>
    <link>http://0.0.0.0:4000/</link>
    <atom:link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 20 Apr 2023 04:06:40 -0500</pubDate>
    <lastBuildDate>Thu, 20 Apr 2023 04:06:40 -0500</lastBuildDate>
    <generator>Jekyll v3.7.3</generator>
    
      <item>
        <title>电影聚合网站的建设 前后端建站全过程回顾</title>
        <description>&lt;p&gt;-–&lt;/p&gt;

&lt;p&gt;layout:     post&lt;/p&gt;

&lt;p&gt;title:         电影聚合网站的建设-前后端建站全过程回顾&lt;/p&gt;

&lt;p&gt;subtitle:   Programming Massively Parallel Processors&lt;/p&gt;

&lt;p&gt;date:       2023-04-20&lt;/p&gt;

&lt;p&gt;author:     BY  张建&lt;/p&gt;

&lt;p&gt;header-img: img/post-bg-mma-4.jpg&lt;/p&gt;

&lt;p&gt;catalog: true&lt;/p&gt;

&lt;p&gt;tags:&lt;/p&gt;

&lt;p&gt;​    - 兴趣爱好&lt;/p&gt;

&lt;p&gt;-–&lt;/p&gt;

&lt;p&gt;​		疫情三年，经历很多了事情，年纪也来到了32岁，这三年熟悉的朋友大部分都离开了北京，自己的心境也发生了很多变化，由焦虑、抑郁逐渐转为目前的自洽，才发现这是一场与自己斗争的战争，幸运的事，这场战争目前我幸存下来了。由于在抗争的过程中，发现自己的时间原来可以去做好多事情，不是只躺在床上打游戏浪费时间，时间真是个好东西。&lt;/p&gt;

&lt;p&gt;​		言归正传，本篇呢，是今年开始逐渐给自己找事做，自己建了一个小站的项目总结。当然，现在本站只是一个小开始，会慢慢优化并丰富内容。大家可以先去目睹一下&lt;a href=&quot;https://yuanqi.plus&quot;&gt;元气&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;

</description>
        <pubDate>Thu, 20 Apr 2023 00:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/2023/04/20/%E7%94%B5%E5%BD%B1%E8%81%9A%E5%90%88%E7%BD%91%E7%AB%99%E7%9A%84%E5%BB%BA%E8%AE%BE-%E5%89%8D%E5%90%8E%E7%AB%AF%E5%BB%BA%E7%AB%99%E5%85%A8%E8%BF%87%E7%A8%8B%E5%9B%9E%E9%A1%BE/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2023/04/20/%E7%94%B5%E5%BD%B1%E8%81%9A%E5%90%88%E7%BD%91%E7%AB%99%E7%9A%84%E5%BB%BA%E8%AE%BE-%E5%89%8D%E5%90%8E%E7%AB%AF%E5%BB%BA%E7%AB%99%E5%85%A8%E8%BF%87%E7%A8%8B%E5%9B%9E%E9%A1%BE/</guid>
        
        
      </item>
    
      <item>
        <title>CUDA-Parallel patterns convolution</title>
        <description>&lt;p&gt;本章主要介绍了卷积作为一种重要的并行计算模式。首先展示了基本的并行卷积算法，其执行速度受到了DRAM带宽的限制。然后提出了tiling kernel。并介绍了一种数据缓存的简化版代码。最后介绍了2D卷积核。
在高性能计算中，卷积通常称为stencil computation。因为每一个输出元素均可以独立计算，且输入数据共享了输出元素，这些特性使得卷积成为复杂tiling 方法与输入数据分布策略的重要应用。
缺失的边界值(padding)通常称为”ghost cells” or “halo cells”。这些cell值对tiling的高效性。&lt;/p&gt;

&lt;h2 id=&quot;1d-parallel-convolutional-a-basic-algorithm&quot;&gt;1D Parallel Convolutional-A Basic Algorithm&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;__global__ void convolution_1D_basic_kernel(float *N, float *M, float *P, int Mask_Width, int Width){
//kenel body
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;假设Mask_width是奇数，卷积对称。例如：Mask_Width = 2*n + 1,则P[i]的输出值将利用N[i-n], N[i-n+1],…，N[i],N[i+1],N[i+n-1],N[i+n]。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;float Pvalue = 0;
int N_start_point = i - (Mask_Width/2);
for(int j =0;j &amp;lt; Mask_Width; j++)
{
if(N_start_point + j &amp;gt;=0 &amp;amp;&amp;amp; N_start_point + j &amp;lt; Width){
Pvalue += N[N_start_point + j] *M[j];
}
}
P[i] = Pvalue;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;上述省略了ghost cell与对应N值的乘积
核函数：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;__global__ void convolution_1D_basic_kernel(float *N, float *M, float *P, int Mask_Width, int Width){

int i = blockIdx.x * blockDim.x + threadIdx.x;
float Pvalue = 0;
int N_start_point = i - (Mask_Width / 2);
for(int j =0;j &amp;lt; Mask_Width;j++){
if(N_start_point + j &amp;gt;= 0 &amp;amp;&amp;amp; N_start_point + j &amp;lt; Width){
Pvalue += N[N_start_point + j]*M[j];
}
}
P[i] = Pvalue;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;1D convolution kernel with boundary condition handling
分析：上述会有control flow divergence. 其代价视输入数组的宽度与mask的Mask_Width的不同而变化。
比较严重的问题是内存带宽。这里浮点数学计算对全局内存的访问比值才为1.&lt;/p&gt;
&lt;h2 id=&quot;constant-memory-and-caching&quot;&gt;Constant Memory and Caching&lt;/h2&gt;
&lt;p&gt;滤波器数组M的三个有趣属性。1是M一般很小。2是M的元素在程序执行时不变。3是所有的线程都以相同顺序访问M的元素。这些特性使得Mask array非常适合constant memory与caching。
constant memory在核函数执行过程中不会改变，比较小，当前只有64KB.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#define Max_MASK_WIDTH 10
__constant__ float M[Max_MASK_WIDTH];
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;transfer code&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cudaMemcpyToSymbol(M, M_h, Mask_Width * sizeof(float));
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;consant memory变量作为全局变量传给核函数。
为了理解应用constant memory的益处，我们需要更多的理解现代处理器内存与cache hierarchies。
在第5章中，我们知道长延时与DRAM的限制性的带宽是现在处理器的主要瓶颈。为了消除内存瓶颈，现代处理器通常应用片上高速缓存，来减少需要从主内存(DRAM)上访问的变量数量。如下图：
&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1563888676/blog/CUDA/7part/9_mva9pq.jpg&quot; alt=&quot;7-1&quot; /&gt;
处理器将自动在高速缓存中保留最常用的变量并记住它们的DRAM地址。之后当变量用到时，高速缓存中将复制变量。
L1,L2层级高速缓存。L1速度上应该是最快的。
其中一个设计时需要考虑的就是cache coherence。一级缓存修改数据时，其他层级的缓存时不容易发现的。所以需要cache coherence mechanism。大部分CPU均支持。GPU为了增加处理器吞吐量，一般没有该机制。
Constant memory因为在执行过程中不允许修改变量，所以也不需要关注cache coherence。&lt;/p&gt;
&lt;h2 id=&quot;tiled-1d-convolution-with-halo-cells&quot;&gt;Tiled 1D Convolution with Halo Cells&lt;/h2&gt;
&lt;p&gt;这里每一个block处理的输出元素的集合称为output tile。下图展示了用4个block（每个block4个线程）处理1D卷积的例子。在实践中，每个block至少要有32个线程。我们假设M个元素存储在constant memory中。
&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1563888676/blog/CUDA/7part/10_g9qohj.jpg&quot; alt=&quot;7-2&quot; /&gt;
为了减少全局内存的总体访问数量，我们将讨论两种输入数据的tiling策略。第一种是将所有需要计算一个线程block中的输出元素的输入数据加载进共享内存中。需要加载的输入元素大小取决于mask的大小。这里我们假设size = 2 * n + 1。上图中，Mask_Width = 5, n = 2。
Block 0中的线程计算P[0]到P[3]的输出元素。输入元素则需要N[0]到N[5]。另外，还需要N[0]左侧的两个ghost cell元素。其默认值为0。Tile 3右侧也是同样的情形。这里，我们将类似Tile 0 与Tile 3这种成为&lt;strong&gt;边界tiles&lt;/strong&gt;。
Block 1中的线程计算输出元素P[4]到P[7]。他们需要输入元素N[2]到N[9]。元素N[2]到N[3]属于两个tiles并且两次被加载到共享内存中。一次是Block[0],一次是Block[1]。每个block中共享内存的内容仅对自己的线程可见，所以需要被加载进各自的共享内存。像这种被多个tiles涉及并加载多次的元素称为&lt;strong&gt;halo cells或者skirt cells&lt;/strong&gt;。输入tile中仅被一个block应用的部分称为&lt;strong&gt;internal cells&lt;/strong&gt;。
下面将展现将输入tile加载进共享内存的核函数代码。
首先定义共享内存数组N_ds以保存每个block的tile。确保N_ds足够大。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;__shared__ float N_ds[TILE_SIZE + MAX_MASK_WIDTH - 1];
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;然后，我们加载左边的halo cells。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;int halo_index_left = (blockIdx.x - 1) * blockDim.x + threadIdx.x; // Line 1
if(threadIdx.x &amp;gt;= blockDim.x - n){
N_ds[threadIdx.x - (blockDim.x - n)] = 
(halo_index_left &amp;lt; 0)? 0:N[halo_index_left]; // Line 2
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;上述代码Line 1将线程映射到之前tile的元素索引。然后挑选最后的n个线程加载需要的左边halo元素。例如blockDim.x = 4，则只有2和3线程用到了。
Line 2则用于检查halo cells是否是ghost cells。
下一步则是加载输入tile的中间cells。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;N_ds[n + threadIdx.x] = N[blockIdx.x * blockDim.x + threadIdx.x];
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;N_ds的头n个元素包含了left halo cells，则中间元素需要加载进N_ds的下一部分。
然后是加载右边的halo元素,下面我们加载下一个tile的头n个元素。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;int halo_index_right = (blockIdx.x + 1)*blockDim.x + threadIdx.x;
if(threadIdx.x &amp;lt; n){
N_ds[n + blockDim.x + threadIdx.x] = 
(halo_index_right &amp;gt; width ) ? 0: N[halo_index_right];
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;N_ds现在存储有所有的输入tile。每一个线程将利用N_ds的不同部分计算对应的P值。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;float Pvalue = 0;
for(int j = 0; j &amp;lt; Mask_Width; j++){
Pvalue += N_ds[threadIdx.x + j] * M[j];
}
P[i] = Pvalue;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;这里需要加__syncthreads()以确保相同block的所有线程已经加载完毕所需元素。
&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1563888676/blog/CUDA/7part/11_ayrsrc.jpg&quot; alt=&quot;7-3&quot; /&gt;
tiled 1D卷积版本比正常卷积核函数更加复杂，这里引进了额外的算法复杂度以减少DRAM访问的次数，目的则是提高访问计算比。
原文中关于优化访问量的分析，请读原文。&lt;/p&gt;
&lt;h2 id=&quot;a-simpler-tiled-1d-convolution-general-caching&quot;&gt;A Simpler Tiled 1D Convolution-General Caching&lt;/h2&gt;
&lt;p&gt;L1对于每个流处理器是私有的，L2对于所有的流处理器是共享的。该特性可以使我们利用halo cells存在L2缓存中这一特点。
一个block的halo cell值可能是邻近block的内部cell。Tile 1中的halo cell N[2]和N[3]则是Tile 0的内部cell。&lt;strong&gt;Block 1需要利用这些值，但是由于Block 0的访问，它们已经存在于L2 缓存中。&lt;/strong&gt;因此我们可以不用将这些值加载进N_ds。下面将呈现更简单的1D 卷积，其仅加载每个tile的内部元素进入共享内存。
在更简单的tiled kernel中，共享内存N_ds数组只需要加载tile的内部元素。因此大小为声明为TILE_SIZE，而不是TILE_SIZE + MASK_WIDTH -1.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;__shared__ float N_ds[TILE_SIZE];
N_ds[threadIdx.x] = N[blockIdx.x * blockDim.x + threadIdx.x];
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This_tile_start_point与Next_tile_start_point控制了边界，边界条件内，则用N_ds,否则从N中取值。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;__global__ void convolution_1D_tiled_caching_kernel(float* N, float* P,
int Maks_Width, int Width)
{
int i = blockIdx.x * blockDim.x + threadIdx.x;
__shared__ float N_ds[TILE_SIZE];

N_ds[threadIdx.x] = N[i];
__syncthreads();
int This_tile_start_point = blockIdx.x * blockDim.x;
int Next_tile_start_point = (blockIdx.x + 1) * blockDim.x;
int N_start_point = i - (Mask_Width / 2);
for(int j = 0; j &amp;lt; Mask_Width; j++)
{
 int N_index = N_start_point + j;
 if(N_index &amp;gt; 0 &amp;amp;&amp;amp; N_index &amp;lt; Width)
 {
 if((N_index &amp;gt; This_tile_start_point) &amp;amp;&amp;amp; (N_index &amp;lt; Next_tile_start_point))
 {
 Pvalue += N_ds[threadIdx.x + j - (Mask_width / 2)] * M[j];
 }else{
 Pvalue += N[N_index] * M[j];
 }
 }
}
P[i] = Pvalue;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Thu, 08 Aug 2019 00:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/2019/08/08/CUDA-Parallel-patterns-convolution/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2019/08/08/CUDA-Parallel-patterns-convolution/</guid>
        
        <category>CUDA</category>
        
        
      </item>
    
      <item>
        <title>CUDA-Performance considerations</title>
        <description>&lt;p&gt;性能考量&lt;/p&gt;

&lt;p&gt;在不同的应用程序中，由于不同因素，经常会遇到性能瓶颈。我们需要考虑清楚是算法策略还是硬件因素的影响，基于这个认识，本章将给出算法模式的类型对高性能表现的影响。&lt;/p&gt;
&lt;h2 id=&quot;全局内存带宽&quot;&gt;全局内存带宽&lt;/h2&gt;
&lt;p&gt;影响CUDA Kernel表现的主要因素之一是对全局内存的数据访问。本节将介绍Memory coalescing技术，该技术经常与tiling技术一起应用以高效利用全局内存带宽。
CUDA设备中全局内存的实现是由DRAMs技术实现的。数据位存储在叫做DRAM cell的小电容器中。
现代DRAM用并行技术提高数据访问率，通常叫做内存访问吞吐量。
通常电容器越大，速度越快。但是现在每单元电容器的体积越来越小，所以速度并没有提升。
每次访问DRAM一个位置，实际上访问了小范围的连续位置。DRAM上传感器都是并行工作的，每一个连续位置内的传感器都存储了数据的一位。一旦访问到一个，其他的都可以高速传输到处理器中。这种方式称为DRAM bursts。如果我们可以充分利用这种特性，DRAM则可以很快的提供数据，这比随意访问任意位置高效的多。
认识到DRAM的burst特性后，当前GPU
允许编程者将线程访问组织成适当形式以实现高效数据访问。这利用了warp线程中执行相同指令的特点。例如当线程0访问N位置，线程1访问N+1，等，则他们会被固化成单一指令进行访问。&lt;/p&gt;

&lt;h2 id=&quot;53-warps-and-simd-hardware&quot;&gt;5.3 Warps and SIMD HardWare&lt;/h2&gt;
&lt;p&gt;当前的CUDA设备会捆绑线程执行程序，此种策略会因为代码的类型造成表现差异。
&lt;strong&gt;第三章，有介绍每一个线程块被分成了warps.Warps的执行是通过SIMD硬件执行的。这种技术可以减少硬件制作成本，低电力消耗以及实现固化内存访问。然而在实现过程中，warp的大小很容易改变。当前，所有的CUDA设备warp配置几乎都设置为每个warp包含32个线程。&lt;/strong&gt;
&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1561959731/blog/CUDA/v2_c1crjs.jpg&quot; alt=&quot;display&quot; /&gt;
###说明：
将线程组织成warp的动机如上图所示：每一个处理器有一个控制单元可以取并解码指令。相同的控制信号进入多个处理单元，每一个都执行了一个warp的线程中的一个。因为所有的处理单元被相同的指令控制，则执行结果的不同是因为register file中存储的不同数据。这在处理器设计中就叫做Single-Instruction-Multiple-Data(SIMD).
现代处理器中的控制单元相当复杂。多个处理单元共享一个控制单元可以减小硬件制作成本以及电力消耗。这种设计在未来处理器中已成为一种趋势。
对于一个block的大小不是32的倍数，最后的warp将被其他线程填充以凑够32个线程。
对于二维数组，以行展开，组成warp.三维则以x,y组成二维数组，按z方向展开。
在if-else的例子中，相同warp中的线程会执行不同的执行路径，我们定义这种情况为线程的diverge。Thread Divergence 会严重影响程序的执行时间，因为它们是串行执行。
&lt;strong&gt;如果一个循环条件是建立在线程的索引序号上，那么就会发生线程的divergence.&lt;/strong&gt;
&lt;strong&gt;一个广泛发生线程divergence的原因是在映射线程到数据，处理边界条件时。&lt;/strong&gt;不过，对表现的影响随着数据的大小发生变化。通常，向量越大，随表现的影响越小。（32的倍数，长度为100的向量，1/4个warp会D，1000，则1/32。）
&lt;strong&gt;在一些重要的并行算法中，Control divergence会随着参与计算的线程的数量的增加而增加。下面将以reduction algorithm为例进行介绍&lt;/strong&gt;
&lt;strong&gt;reduction算法从数组值中取出一个值。该值可以是和，最大值胡总和最小值。所有这些类型的reduction算法的计算结构都相同。&lt;/strong&gt;首先，可以通过串行计算实现，每个元素均被访问一次，算法的执行时间与元素数量成正比。算法复杂度为O(N)。
因此，可以想到通过并行计算以减少执行时间。可以类比世界杯足球比赛。可以想象1024个队伍，通过10轮就可以得出最后的胜利者，前提是需要有足够的球场。
下图展现了实现并行求和reduction的核函数。原始数组存储在全局内存中。每一个线程块减少了数组的一部分，通过将该部分的元素加载进共享内存并在这些元素上执行并行reduction。代码将输入数组X中的元素从全局内存中加载进共享内存中。reduction是原地进行的，这就意味着共享内存的一些元素会被部分求和值替代。核函数的for循环中，每一个迭代都会实现一轮reduction。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;__shared__ float partialSum[SIZE];
partialSum[threadIdx.x] = X[blockIdx.x * blockDim.x + threadIdx.x];
unsigned int t = threadIdx.x;
for(unsigned int stride = 1; stride &amp;lt; blockDim.x; stride *= 2)
{
__syncthreads();
if(t % (2 * stride) == 0)
    partialSum[t] += partialSum[t + stride];
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;__syncthreads()确保了for-loop循环中之前迭代的所有部分和值已经被产生了并且之前的任意一个线程被允许开始当前迭代。此种方式下，所有进入第二次迭代的线程将利用第一次迭代产生的值。在第一轮迭代后，偶数元素将被第一轮的求和值替代掉。
第二轮迭代后，元素下标索引的4的倍数的元素将被求和值替换掉。最后一轮，整个数组的总和将在partialSum[0]得到。
for循环中，stride首先初始化为1.在第一次迭代时，if条件将选取偶数线程执行两个相邻元素的加法。下图展示了核函数的执行。线程与数组元素以水平方向展示。由线程执行的迭代从上到下在竖直方向展示。每一行则展示了迭代后数组的元素内容。
&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1563638843/blog/CUDA/5-1_qp7fsz.jpg&quot; alt=&quot;5-1&quot; /&gt;
Divergence分析:
在第一轮迭代中偶数线程将执行加法操作。在接下来的迭代中，更少的线程将执行if之后的加法操作，但是所有的线程在每一轮迭代中仍将执行判断操作。此divergence可以通过对算法的轻微改变实现性能提升。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;__shared__ float partial[SIZE]
partialSum[threadIdx.x] = X[blockIdx.x + blockDim.x * blockIdx.x];
unsigned int t = threadIdx.x;
for(unsigned int stride = blockDim.x / 2; stride &amp;gt;=1; stride = stride&amp;gt;&amp;gt;1)
{
__syncthreads();
if(t &amp;lt; stride)
    partialSum[t] += partialSum[t + stride]; // Line 7
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;修改版本将stride初始化为数组长度的一半，表现差异性分析表现：
两者为什么会有不同呢？答案就在执行Line 7的线程与不执行Line 7的线程，他们的位置差异。
在第一轮迭代中，所有线程索引小于数组长度一半的threadIdx.x执行了Line7. 对于一个512个元素的数组， 第一轮迭代中，Thread 0到255执行了加法，256到511没有。在第一轮迭代之后，两个元素的加和存储在了元素0到255中。由于warp中32个线程，也就是warp 0到warp 7执行了加法，而8到15则略过了。由于每个warp中的所有线程都执行了相同指令，所以并没有divergence发生。
读者应该注意到，此图中并没有完全消除divergence。在5th个循环开始，执行Line7的线程将小于32.但是这也将发生divergence的循环由10降到了5.
&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1563638904/blog/CUDA/5-2_f6wtoo.jpg&quot; alt=&quot;5-2&quot; /&gt;
两图中虽然改变很小，但是造成的算法表现差异却很大。这需要编程者很好的理解SIMD硬件中线程的执行，从而作出相应的调整。&lt;/p&gt;
</description>
        <pubDate>Wed, 24 Jul 2019 00:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/2019/07/24/CUDA-%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98%E5%B8%A6%E5%AE%BD/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2019/07/24/CUDA-%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98%E5%B8%A6%E5%AE%BD/</guid>
        
        <category>CUDA</category>
        
        
      </item>
    
      <item>
        <title>深度学习模型性能影响分析</title>
        <description>&lt;p&gt;http://machinethink.net/blog/how-fast-is-my-model/
在移动端部署深度学习模型时，准确率并不是唯一需要衡量的因素，还需要考虑以下4个方面：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;模型占用的app内存空间大小-一个模型可能给app增加100MB。&lt;/li&gt;
  &lt;li&gt;运行时占用的内存大小-iPhone和iPad的GPU会占用设备的全部RAM，但是只有几GB。&lt;/li&gt;
  &lt;li&gt;模型运行的速度-特别是当运行实时视频或者大的图像时。&lt;/li&gt;
  &lt;li&gt;耗电情况
测量模型速度的最好方式就是运行多次，取平均值。
案例研究：
作者的一个客户用MobileNetV2的层代替了MobileNetV1中的层。V2比V1用了更少的计算量，理论上V2的速度会快很多，但是V2却慢多了。
下面是通过数学分析原因
1.计算：
衡量模型的计算量，可以用FLOPS(每秒计算的浮点数)，另外一个是MACCS(multiply-accumulate operations)也叫做MADDs.
说明：
衡量模型的计算量可以让你大概了解你的模型的计算消耗，但是其他因素比如内存带宽也非常重要。
    &lt;h2 id=&quot;从头到尾其实就是点积运算&quot;&gt;从头到尾其实就是点积运算&lt;/h2&gt;
    &lt;p&gt;神经网络模型的大部分计算其实是点积运算，比如：&lt;/p&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y = w[0]*x[0] + w[1]*x[1] + w[2]*x[2] + ... + w[n-1]*x[n-1]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;w, x是向量，y是标量。
对于神经网络中的卷积层或者全连接层而言，w就是学习权重，x则是层的输入。
y是层的输入。通常1个层会有多个输出。
我们将w[0]*x[0] + …视为一个MACC（乘法与加法）。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上述方程为n MACCs.
上述点积运算执行了2n -1 FLOPs,即n个乘法与n-1加法。
所以一个MACC大概是两倍的FLOPS。
下面看如何计算不同层的MACCs.&lt;/p&gt;
&lt;h2 id=&quot;全连接层&quot;&gt;全连接层&lt;/h2&gt;
&lt;p&gt;全连接层中，所有的输入与输出相连。对于输入为I，输出为J，权重W存储在I * J的矩阵中。全连接执行的计算为&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y = matmul(x, W) + b
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;x 为I的向量。
点积运算是I中矩阵每行与J中的列相乘，所以共有I* J个MACCs.b被近似掉。
说明：以上假设batchsize为1.&lt;/p&gt;
&lt;h2 id=&quot;激活函数&quot;&gt;激活函数&lt;/h2&gt;
&lt;p&gt;例如ReLu,sigmoid.由于他们不是点积运算，所以只能用FLOP衡量，但是他们的运算量也只是占用了极少的一部分，所以可以忽略掉。&lt;/p&gt;
&lt;h2 id=&quot;卷积层&quot;&gt;卷积层&lt;/h2&gt;
&lt;p&gt;输入FeatureMap: H * W * C
Kernel Size: K
MACCs数量：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;K × K × Cin × Hout × Wout × Cout
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;忽略了激活和bias。stride不可忽略。
对于3 * 3，128个filter，64个channel 的112 * 112的feature map：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;3 × 3 × 64 × 112 × 112 × 128 = 924,844,032
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;此例：stride = 1 padding = “same”&lt;/p&gt;

&lt;h2 id=&quot;depthwise-separable-convolution&quot;&gt;Depthwise-separable convolution&lt;/h2&gt;
&lt;p&gt;这些是MobileNet的基础，稍微大点的模型是Xception。
首先是depthwise卷积，其总的MACCs数量为：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;K × K × C × Hout × Wout
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;例如：一个3&lt;em&gt;3的depthwise卷积，作用于64通道112&lt;/em&gt;112的feature map：MACCs:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;3 × 3 × 64 × 112 × 112 = 7,225,344
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;说明：卷积部分，filter的数量与输入通道的数量一样，每一个filter仅作用在一个通道上。所以与普通卷积相比，没有128.
说明：论文中还有“depthwise channel multiplier”，如果此参数大于1，则每一个输入通道，会有D个输出。即每个输入通道，会有D个filter.
“Separable”部分，此部分为1*1的正常卷积，也叫做“pointwise”卷积。
MACCs:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Cin × Hout × Wout × Cout
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;例子：输入：112 * 112 * 64
输出：112 * 112 * 128
总体MACCs:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;3×3 depthwise          : 7,225,344
1×1 pointwise          : 102,760,448
depthwise separable    : 109,985,792 MACCs

regular 3×3 convolution: 924,844,032 MACCs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;减少了8.4倍。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(K × K × Cin × Hout × Wout) + (Cin × Hout × Wout × Cout)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;简化为：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Cin × Hout × Wout × (K × K + Cout)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;MobileNet V2应用了“expansion block”包含了下列三层：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;1*1 的卷积给feature map增加了更多的channel(叫做”expansion”层)&lt;/li&gt;
  &lt;li&gt;3 * 3的depthwise卷积过滤数据&lt;/li&gt;
  &lt;li&gt;1 * 1的卷积减少通道数量(projection layer，act as bottleneck convolution)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上述expansion block的MACCs的计算公式：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Cexp = (Cin × expansion_factor)

expansion_layer = Cin × Hin × Win × Cexp

depthwise_layer = K × K × Cexp × Hout × Wout

projection_layer = Cexp × Hout × Wout × Cout
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;整合：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Cin × Hin × Win × Cexp + (K × K + Cout) × Cexp × Hout × Wout
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;stride =1 简化为：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(K × K + Cout + Cin) × Cexp × Hout × Wout
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;对比V1:输入112 * 112 * 64， expansion factor：6，
stride = 1的3 * 3 depthwise convolution, 输出通道：128，总体MACCs:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(3 × 3 + 128 + 64) × (64 × 6) × 112 × 112 = 968,196,096
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;这比普通的卷积计算量还要多，但是在block内部，我们实际计算的64 * 6 = 384个channnel。这比普通卷积要多的多。&lt;/p&gt;

&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch normalization&lt;/h2&gt;
&lt;p&gt;Batch normalization对输出层每个元素应用下列公式：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;z = gamma * (y - mean) / sqrt(variance + epsilon) + beta
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;每个通道均有自己的gamma, beta, mean和variance。
每个卷积层需要学习C * 4个参数。
通常batch normalization被应用于ReLU之前，这样，我们可以通过数学运算使batch norm layer消失。
convolution与batch均为线性变换，&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;z = gamma * ((x[0]*w[0] + x[1]*w[1] + ... + x[n-1]*w[n-1] + b) - mean) 
      / sqrt(variance + epsilon) + beta
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;整合则有：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;w_new[i] = w[i]       * gamma / sqrt(variance + epsilon)
b_new    = (b - mean) * gamma / sqrt(variance + epsilon) + beta
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;则，新的卷积运算为：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;z = x[0]*w_new[0] + x[1]*w_new[1] + ... + x[n-1]*w_new[n-1] + b_new
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;所以我们可以忽视batch norm layer的影响。
说明：仅convolution, batch norm, ReLU顺序时有效。&lt;/p&gt;
&lt;h2 id=&quot;memory&quot;&gt;Memory&lt;/h2&gt;
&lt;p&gt;内存带宽其实比计算更重要。
在当前并行计算机架构下，单次内存获取比单词计算要慢的多-大概在100倍或者更多。
每一层，设备需要：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;从主机内存中读取输入向量&lt;/li&gt;
  &lt;li&gt;计算点积-从主机内存中读取权重&lt;/li&gt;
  &lt;li&gt;将新向量或者特征写回主内存。
涉及的内存访问很多，所以会严重影响速度。
    &lt;h2 id=&quot;权重的内存占用&quot;&gt;权重的内存占用&lt;/h2&gt;
    &lt;p&gt;一般来讲，模型的权重越少，模型运行越快。&lt;/p&gt;
    &lt;h2 id=&quot;feature-map-和中间结果&quot;&gt;Feature map 和中间结果&lt;/h2&gt;
    &lt;p&gt;对于224 * 224 * 3的输入，全部读取需要150528次内存访问。
如果卷积核为3 * 3，则需要对每个元素读取9次得到输出。
由于有Cout个卷积核，则每个输入元素需要读取K * K * Cout次。
如果这个卷积层的stride为2, 32个filter，则会写入112 * 112 * 32个值，内存访问次数401408.
一般，每一层的内存访问次数为：&lt;/p&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input = Hin × Win × Cin × K × K × Cout
output = Hout × Wout × Cout
weights = K × K × Cin × Cout + Cout
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;这里假设权重只读取一次。
假设256的input, 512的输出。&lt;/p&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input = 28 × 28 × 256 × 3 × 3 × 512 = 924,844,032
output = 28 × 28 × 512 = 401,408
weights = 3 × 3 × 256 × 512 + 512 = 1,180,160
total = 926,425,600
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;对depthwise-separable，3 * 3的depthwise 1* 1的pointwise，则
```
depthwise layer
input = 28 × 28 × 256 × 3 × 3 = 1,806,336
output = 28 × 28 × 256 = 200,704
weights = 3 × 3 × 256 + 256 = 2,560
total = 2,009,600&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;pointwise layer
input = 28 × 28 × 256 × 1 × 1 × 512 = 102,760,448
output = 28 × 28 × 512 = 401,408
weights = 1 × 1 × 256 × 512 + 512 = 131,584
total = 103,293,440&lt;/p&gt;

&lt;p&gt;total of both layers = 105,303,040&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;可以看到计算量少8.4倍，访问量大概也少8.8倍。
# Fusion
对于ReLu,应用于28 * 28 * 512,有
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;input = 28 × 28 × 512 = 401,408
output = 28 × 28 × 512 = 401,408
total = 802,816&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;此部分可以与卷积层融合，以减少内存访问。
## MobileNet V2 versus V1
之前我们提到Mobilenet V2(with depth multiplier 1.4)跟V1运行的速度差不多，虽然它的参数更少。
此具体的实际使用案例为：用MobileNet作为特征提取器，
V1截止到conv_pw_11(共23层),v2则到expanded_conv_12(共47层)。
输入图像为126 * 224，是720 * 1280摄像机的缩小版。
参数量
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;MobileNet V1 parameters (multiplier = 1.0): 1.6M
MobileNet V2 parameters (multiplier = 1.0): 0.5M
MobileNet V2 parameters (multiplier = 1.4): 1.0M&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;MACCs数量
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;MobileNet V1 MACCs (multiplier = 1.0): 255M
MobileNet V2 MACCs (multiplier = 1.0): 111M
MobileNet V2 MACCs (multiplier = 1.4): 214M&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;以上两者差不多
内存访问对比：
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;MobileNet V1 memory accesses (multiplier = 1.0): 283M
MobileNet V2 memory accesses (multiplier = 1.0): 159M
MobileNet V2 memory accesses (multiplier = 1.4): 286M&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;multiplier = 1.4的版本跟v1比，内存访问几乎没区别。

VGG16也作为特征提取器
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;VGG16 parameters:        15M
VGG16 MACCs:           8380M
VGG16 memory accesses: 8402M
```
虽然VGG层数更少，但是它执行操作的feature map很大并且它的内存访问相当大。
需要注意的是，以上比较只有在两个模型在相同软件架构下，相同的硬件上进行比较才有意义。&lt;/p&gt;
</description>
        <pubDate>Fri, 12 Jul 2019 00:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/2019/07/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%A7%E8%83%BD%E5%BD%B1%E5%93%8D%E5%88%86%E6%9E%90/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2019/07/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%A7%E8%83%BD%E5%BD%B1%E5%93%8D%E5%88%86%E6%9E%90/</guid>
        
        <category>CUDA</category>
        
        
      </item>
    
      <item>
        <title>CUDA-Memory and data locality</title>
        <description>&lt;p&gt;本章总结了共享内存的用法以及注册器可以影响每个流多处理器能容纳的线程BLOCK的数量—memory bound.
迄今为止我们了解的知识只是挖掘了潜在硬件性能的很小一部分.程序的糟糕表现主要是由于全局内存的长访问延迟和有限的获取带宽,通常是由Dynamic Random Access Memory.(DRAM).
内存高效访问的重要性
compute-to-global-memory-access（CGMA） is defined as the number of floating-point calculation performed for each access to the global memory within a region of progam.
浮点计算对应全局内存获取的比值
computer-to-global-memory-access对CUDA核的表现有重要影响.程序的执行速度被内存获取吞吐量限制了.为了获得更高的表现,我们需要通过减少全局内存的获取来增加ratio.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;__global__ void MatrixMulkernel(float* M, float* N, float* P, int Width){
int Col = blockIdx.x * blockDim.x + threadIdx.x;
int Row = blockIdx.y * blockDim.y + threadIdx.y;

if( (Row &amp;lt; Width) &amp;amp;&amp;amp; (COl &amp;lt; Width))
{
float Pvalue = 0;
for(int k = 0; k &amp;lt; Width; ++k){
Pvalue += M[Row * Width + k] *N[K* Width + Col];
}
P[Row * Width + Col] = Pvalue;
}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;对于矩阵运算,矩阵M和N对元素的获取是两次全局内存访问对应一次浮点加法与乘法.比率为1.此比率将导致现代GPU少于峰值执行速度利用率的2%.我们需要增加设备的计算吞吐量
Registers and shared memory(寄存器与共享内存)是on-chip memories。存储在这两者上的变量可以高速并行的方式获取。每一个线程拥有自己的寄存器。核函数通常用寄存器存储高频访问的线程私有变量。共享内存被分配给线程块。一个线程块中的所有内存都可以获取到共享内存变量。共享内存是线程协作的高效方式。
&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1561959180/blog/CUDA/memory_ajar5z.jpg&quot; alt=&quot;display&quot; /&gt;&lt;/p&gt;

&lt;p&gt;内存类型的了解
&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1561959731/blog/CUDA/v1_o4wovv.jpg&quot; alt=&quot;display&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Globay memory在处理器芯片外，由DRAM技术实现，意味着长时间访问延迟和低访问带宽。Register File通常在寄存器上，意味着很短的访问延迟和极高的访问带宽。在通常设备上，访问带宽的性能是Global memory的至少两倍数量级。当变量存储在Register上时，它的访问不再消耗global memory。减少的带宽消耗将通过增加的CGMA比值所反映出来。&lt;/li&gt;
  &lt;li&gt;操作更少，因为不用再执行加载。&lt;/li&gt;
  &lt;li&gt;从Register上访问变量消耗的能量也比global至少少一个数量级。但是每个线程可用的Register数量相当受限。
共享内存与Register同属于片上内存。但是共享内存需要执行内存加载操作，所以性能上Register &amp;gt; shared memory &amp;gt; global memory
&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1561959731/blog/CUDA/v2_c1crjs.jpg&quot; alt=&quot;display&quot; /&gt;
一个重要不同就是block块中的所有线程可以访问共享内存中的变量，但是Register中为线程的私有变量。
&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1561963012/blog/CUDA/memory_type_dugzde.png&quot; alt=&quot;display&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;1.标量(非数组或矩阵变量)在核函数或者device function中被分类到Register中。变量blurRow, blurCol,curRow,curCol,pixels,pixVal均归到此类。需要注意的是Register的存储容量，不要超出限制。大量的Register会严重影响分配给每个SM的活跃线程数量。
2.数组变量分配在global memory中。在核函数与设备函数中很少用。&lt;/p&gt;

&lt;p&gt;“&lt;strong&gt;constant&lt;/strong&gt;“代表了常量内存。常量内存必须在函数体外。其生命周期就是程序的生命周期。其存储在全局内存中，但是为了高效访问是以缓存形式存在的。常量内存大小限制在65536个字节。所以输入需要分开喂入程序，如第7章卷积部分所展示的那样。
变量声明前只有”&lt;strong&gt;device&lt;/strong&gt;“的是一个全局变量并且将会放置在全局内存中。对全局内存的访问很慢。其生命周期也是伴随着整个应用进程。因此其可以用来在不同线程块之间进行协作。不同线程之间需要同步来确保信息的一致性。
在CUDA中，指针用来指向全局内存中的数据对象。在核函数和设备函数中指针的用法有两种方式，一是对象都有host function创建，则由cudaMalloc()初始化，并作为参数传递给核函数。二是在全局内存中声明的变量地址被赋值给指针变量。读者应该参考CUDA 编程文档来参考其他的内存类型。&lt;/p&gt;

&lt;h2 id=&quot;44-tiling-for-reduced-memory-traffic&quot;&gt;4.4 Tiling for Reduced Memory Traffic&lt;/h2&gt;
&lt;p&gt;全局内存大但是慢，而共享内存小但是快。通用的策略是将数据分成叫做tiles的子集，这样每一个tile可以适应共享内存。一个重要的标准是在这些tiles上的核函数计算可以相互独立执行。需要说明的是给定任意核函数，并不是所有数据结构可以分成tiles.
下面将通过4.5中矩阵乘法的例子来解释&lt;strong&gt;tiling&lt;/strong&gt;的概念.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1561988864/blog/CUDA/4-8_wwahaj.jpg&quot; alt=&quot;display&quot; /&gt;
此示例假设我们用2 * 2 的block来计算P矩阵. 下图展示了由block(0, 0)的4个线程执行的计算过程.这4个线程分别计算P(0,0),P(0,1),P(1,0)和P(1,1).由block(0, 0)的线程(0,0)和线程(0, 1)访问的M与N中的元素由黑色箭头进行标示.例如thread(0, 0)读取M(0,0),N(0,0),M(0,1),N(1,0),M(0,2),N(2,0),M(0,3),N(3,0).
&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1561989324/blog/CUDA/4-9_nsj6ga.jpg&quot; alt=&quot;display&quot; /&gt;
图4.10展示了block(0,0)中所有线程访问的全局内存.表格中竖直方向表示了线程,水平方向展示了随时间增加对矩阵中元素的访问顺序.每一个线程在执行过程中分别访问矩阵M与N的4个元素.但是在4个线程中,线程访问之间有很多的重叠部分.例如,thread(0,0)与thread(0,1)都访问了M(0,0)以及M中行位置是0的元素.
&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1561989348/blog/CUDA/4-10_jglayl.jpg&quot; alt=&quot;display&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果thread(0, 0) 与thread(0, 1)可以协同工作,那么M中的元素就可以只加载一次.全局内存总的访问量可以降低一半.&lt;/p&gt;

&lt;p&gt;In the context of parallel computing, tiling is a program transformation technique that localizes the memory locations accessed among threads and the timing of their accesses. It divides the long access sequences of each thread into phases and uses barrier synchronization to keep the timing of accesses to each section at close intervals.&lt;/p&gt;

&lt;p&gt;现在将介绍tiled矩阵乘法算法.基本的想法是在他们单独计算内积计算时,线程协作加载M与N元素的子集到共享内存.将M与N划分成较小的tiles,可以使它们加载进共享内存.最简单的形式就是tile的维度等于block的维度.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1561992258/blog/CUDA/4-13_az1zpl.jpg&quot; alt=&quot;display&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1561992266/blog/CUDA/4-14_hwxlmn.jpg&quot; alt=&quot;display&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图中Phase1中N(0,1)应该对应于Nds(0, 1).&lt;/p&gt;

&lt;p&gt;对于BLOCK(0,0)计算分成了两部分,观察图,可以知道取中间值Pvalue,通过累加实现了计算.&lt;/p&gt;

&lt;p&gt;如果矩阵的宽度为Width, tiles的大小为TILE_WIDTH,则点积的计算将在WIDTH/TILE_WIDTH阶段内完成.&lt;/p&gt;

&lt;p&gt;说明: Mds  和Nds重复利用来保存输入值. This is due to the fact that each phase focuses on a small subset of the input matrix elements. Such focused access behaviour is called locality.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Locality is as important for achieving high-performance in multi-core CPUs as in many-thread GPUs.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;45-a-tiled-matrix-multiplication-kernel&quot;&gt;4.5 A Tiled Matrix Multiplication Kernel&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;__global__ void MatrixMulKernel(float *d_M, float* d_N, float* d_P, int Width){
__shared__ float Mds[TILE_WIDTH][TILE_WIDTH];
__shared__ float Nds[TILE_WIDTH][TILE_WIDTH];

int bx = blockIdx.x; int by = blockIdx.y;
int tx = threadIdx.x; int ty = threadIdx.y;
// Identify the row and column of the d_p element to work on
int Row = by * TILE_WIDTH + ty;
int Col = bx * TILE_WIDTH + tx;
float Pvalue = 0;
for(int ph = 0; ph &amp;lt; Width / TILE_WIDTH; ph++){
	Mds[ty][tx] = d_M[Row * WIDTH +ph* TILE_WIDTH + tx];
	Nds[ty][tx] = d_N[(ty + ph * TILE_WIDTH) * WIDTH + Col];
	__syncthreads();
}
for(int k = 0; k &amp;lt; TILE_WIDTH; k++)
{
pvalue += Mds[ty][k] * Nds[k][tx];
}
__syncthreads();
d_P[Row * Width + Col] = Pvalue;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/dsn0i1fsm/image/upload/v1561996239/blog/CUDA/4-15_bjjdta.jpg&quot; alt=&quot;Display&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以结合上图对函数进行理解,主要是计算坐标的理解.&lt;/p&gt;

&lt;p&gt;第一个__synchreads()确保了所有线程完成了tiles的加载.第二个则确保所有线程用完共享内存的元素.&lt;/p&gt;

&lt;p&gt;Strip-mining技术.取长循环为分阶段实现.如果用16*16的TILE_WIDTH,则可以减少全局内存的访问16倍.即增加CGMA由1到16.&lt;/p&gt;

&lt;p&gt;上述有两个假设,首先矩阵的宽度是BLOCK宽度的倍数.二是矩阵必须是方形矩阵.下面一节将打破这两个假设.&lt;/p&gt;

&lt;h2 id=&quot;46-boundary-checks&quot;&gt;4.6 Boundary Checks&lt;/h2&gt;

&lt;p&gt;我们现在扩展这个矩阵乘法算法到可以处理任意的矩阵宽度.这次扩展可以使核能处理任意宽度不是tile宽度的倍数的情况.下面依然是方形矩阵.我们需要检查输入矩阵的有效情况.边界检查:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Row &amp;lt; WIDTH &amp;amp;&amp;amp; (ph * TILE_WIDTH + tx) &amp;lt; WIDTH)
(ph * TILE_WIDTH + ty) &amp;lt; WIDTH &amp;amp;&amp;amp; Col &amp;lt; WIDTH
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;代码:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for(int ph = 0; ph &amp;lt; ceil(Width / (float)TILE_WIDTH; ++ph)
{
 if((ROW &amp;lt; WIDTH)&amp;amp;&amp;amp;(ph * TILE_WIDTH + tx) &amp;lt; Width)
     Mds[ty][tx] = M[Row * Width + ph * TILE_WIDTH + tx];
 if((ph * TILE_WIDTH + ty) &amp;lt; Width &amp;amp;&amp;amp; Col &amp;lt; Width)
     Nds[ty][tx] = N[(ph*TILE_WIDTH + ty)*Width + Col];
 __syncthreads();
 for(int k = 0; k &amp;lt; TILE_WIDTH; ++k){
  Pvalue += Mds[ty][k] * Nds[k][tx];
 }
 __syncthreads();
}
if((Row &amp;lt; Width) &amp;amp;&amp;amp; (Col &amp;lt; Width) P[Row * Width + Col] = Pvalue;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;下面距离通用的矩阵乘法核还差一步,对于J* K的M矩阵与K * L的N矩阵相乘.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;M: Col &amp;lt; J &amp;amp;&amp;amp; (ph * TILE_WIDTH + tx) &amp;lt; K
N: (ph * TILE_WIDTH + ty) &amp;lt; K &amp;amp;&amp;amp; Col &amp;lt; L
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;47-memory-as-a-limiting-factor-to-parallelism&quot;&gt;4.7 Memory as a Limiting Factor to Parallelism&lt;/h2&gt;
&lt;p&gt;通常每个线程要求的资源越多,每个SM中存在的线程则越少.
cudaGetDeviceProperties(&amp;amp;dev_prop)
dev_prop.regsPerBlock 查看register的数量
对于共享内存也是,对于矩阵乘法来说,对于16 * 16的tile_size, 则每个block需要16 * 16 *4 = 1k字节的存储(说明 每个元素是浮点数, 大小为4个字节).加上Nds, 则需要2K.所以 一个16K的共享内存只允许 同时存在8个block.在这里,真正的限制是线程数量.如果每个SM有1536个线程,则只允许存在6个block.所以只能用6 * 2KB = 12KB的共享内存.
每一代的共享内存数量都不同,我们可能希望代码可以随意调整一个kernel中的共享内存大小. 
通过cudaGetDeviceProperties(&amp;amp;dev_prop), dev_prop.sharedMemPerBlock.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;extern  与size_t需要注意.以后需要回看&lt;/strong&gt;&lt;/p&gt;

</description>
        <pubDate>Mon, 01 Jul 2019 00:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/2019/07/01/CUDA-Memory-and-data-locality/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2019/07/01/CUDA-Memory-and-data-locality/</guid>
        
        <category>CUDA</category>
        
        
      </item>
    
      <item>
        <title>失落的时间-2019半年度学习计划</title>
        <description>&lt;p&gt;由于学习目标是从6月中旬才开始意识到，大概前几年都是荒废了，到现在才大概有点觉悟，自己的懒惰与拖延造成了好几年的学习计划都没有贯彻执行，所以一些学习的计划一直拖延到现在，所以如果再这样下去，即使再过几年，我肯定不会有所长进，大概率会被市场淘汰，最重要的这种习惯影响的不止学习，由此会使生活及人生都会被这种习惯所主导，以至于整个人生都不会有大的起色，这大概从大学到现在差不多10年我的人生轨迹就能看出来，所以从下半年开始，要逐渐有所起色。&lt;/p&gt;

&lt;p&gt;加油吧，把失去的时间找回来。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;如何选择自己的从业方向&lt;/strong&gt;： 
认清自己，找准优势 
结合兴趣，选择方向 
制定计划，不断努力 
阶段检查，查缺补漏&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;1.&lt;/strong&gt; 目前CUDA高性能实战的书基本看完，下面需要看一些实际的图像处理代码和另外一本英文的cuda书。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;2.&lt;/strong&gt; 数据结构算法即数据结构与算法导论的书&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;3.&lt;/strong&gt; c++并发，多线程的书&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;4.&lt;/strong&gt; 三维重建的书，其中矩阵处理，线性代数 
以上本想一本一本的看，目前看来没必要，而且这样会增加无聊的机率，所以选择三本书和几个目标，并行的去看，或者轮询的方式去看。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每天定制课程表，以上4个课程每周1天，并且采用循环的方式。这是总的计划表，二是以上4个课程，又需要分别制定单独的大概计划。 
博客分为4个Tag,分别是cuda,算法导论刷题（此目录待定），C++并发，三维重建。&lt;/p&gt;
&lt;h3 id=&quot;cuda&quot;&gt;cuda&lt;/h3&gt;
&lt;p&gt;porgramming masselively parallel processors读书以及 
caffe源码，nvidia SDK阅读任务&lt;/p&gt;
&lt;h3 id=&quot;算法导论刷题&quot;&gt;算法导论刷题&lt;/h3&gt;
&lt;p&gt;算法导论的书籍选择阅读，算法那本书的编程（java）。 
可以优先阅读java这本书。总之再决定吧。&lt;/p&gt;
&lt;h3 id=&quot;c并发&quot;&gt;C++并发&lt;/h3&gt;
&lt;p&gt;先看C++并发的中文版，这样阅读会加快，并且阻碍会少。后期再考虑英文版本。&lt;/p&gt;
&lt;h3 id=&quot;三维重建&quot;&gt;三维重建&lt;/h3&gt;
&lt;p&gt;继续之前的书，这部分也是较难的，不过还是得继续。将书和代码先看一遍，能做笔记就做笔记，后期再进行实际项目的操作，以熟悉实际过程。 
以上更新到新的学习章程里。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;看的书，博客或者论文需要有笔记的形式记录下来。&lt;/li&gt;
  &lt;li&gt;除了目前补缺内容，养成去掉刷手机娱乐内容的习惯，进而改成看书的习惯。&lt;/li&gt;
  &lt;li&gt;多看一些大牛的博客，总结以及向他人学习。&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Mon, 24 Jun 2019 00:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/2019/06/24/2019%E4%B8%8B%E5%8D%8A%E5%B9%B4%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2019/06/24/2019%E4%B8%8B%E5%8D%8A%E5%B9%B4%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87/</guid>
        
        <category>年度学习计划</category>
        
        
      </item>
    
      <item>
        <title>CUDA - 多GPU系统上的CUDA C</title>
        <description>&lt;h2 id=&quot;本章目标&quot;&gt;本章目标&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;了解如何分配和使用零拷贝内存(Zero-Copy Memory)&lt;/li&gt;
  &lt;li&gt;了解如何在同一个应用程序中使用多个GPU&lt;/li&gt;
  &lt;li&gt;了解如何分配和使用可移动的固定内存(Portable Pinned Memory)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;零拷贝主机内存&quot;&gt;零拷贝主机内存&lt;/h2&gt;
&lt;p&gt;固定内存(页锁定内存),这种新型的主机内存能够确保不会交换出物理内存.我们通过调用cudaHostAlloc()来分配这种主机内存,并且传递参数cudaHostAllocDefault来获得默认的固定内存.本章中,除了默认参数外,我们还可以传递cudaHostAllocMapped.它分配的固定内存除了有前边的属性外,还可以在CUDA C核函数中直接访问这种类型的主机内存.由于这种内存不需要复制到GPU,因此也成为&lt;strong&gt;零拷贝内存&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;通过零拷贝内存实现点积运算&quot;&gt;通过零拷贝内存实现点积运算&lt;/h3&gt;
&lt;p&gt;我们不需要将输入矢量显示复制到GPU,而是使用零拷贝内存从GPU中直接访问数据.&lt;/p&gt;

&lt;p&gt;cudaHostAllocMapped标志告诉运行时将从GPU中访问这块内存.对于两个输入缓冲区,我们还指定了标志cudaHostAllocWriteCombined.这个标志表示,运行时应该将内存分配为合并式写入内存.此功能可以显著提升GPU读取内存时的性能.然而,当CPU也要读取这块内存时,就会很低效.&lt;/p&gt;

&lt;p&gt;在使用标志分配好内存后,就可以从GPU中访问这块内存.然而,GPU的虚拟内存空间与CPU是不同的,因此在GPU上访问它们与CPU上访问它们有着不同的地址.调用cudaHostAlloc()将返回这块内存在CPU上的指针,因此需要调用cudaHostGetDevicePointer()来获得这块内存在GPU上的有效指针.&lt;/p&gt;

&lt;h3 id=&quot;零拷贝内存的性能&quot;&gt;零拷贝内存的性能&lt;/h3&gt;
&lt;p&gt;对于集成GPU,使用零拷贝内存通常都会带来性能提升,因为内存在物理上是与主机共享的.将缓冲区声明为零拷贝内存的唯一目的就是避免不必要的数据复制.但是每个固定内存都会占用系统的可用物理内存,这最终会降低系统性能.
&lt;strong&gt;当输入内存和输出内存只使用一次时,那么在独立GPU上使用零拷贝内存将带来性能的提升.&lt;/strong&gt;如果使用多次,直接将数据复制到GPU.&lt;/p&gt;
</description>
        <pubDate>Sun, 23 Jun 2019 00:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/2019/06/23/%E5%A4%9AGPU%E7%B3%BB%E7%BB%9F%E4%B8%8A%E7%9A%84CUDA-C/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2019/06/23/%E5%A4%9AGPU%E7%B3%BB%E7%BB%9F%E4%B8%8A%E7%9A%84CUDA-C/</guid>
        
        <category>CUDA</category>
        
        
      </item>
    
      <item>
        <title>CUDA - 流(Stream)</title>
        <description>&lt;h2 id=&quot;本章目标&quot;&gt;本章目标&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;了解如何分配页锁定（Page-Locked）的主机内存&lt;/li&gt;
  &lt;li&gt;了解CUDA流的概念&lt;/li&gt;
  &lt;li&gt;了解如何使用CUDA流来加速应用程序&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;malloc()与cudaHostAlloc()分配的内存之间存在着一个巨大的差异。C库函数malloc()将分配标准的，可分页的主机内存。而cudaHostAlloc()将分配页锁定的主机内存。页锁定内存也称为固定内存（Pinned Memory）或者不可分页内存，它有一个重要属性：操作系统将不会对这块内存分页并交换到磁盘上，从而确保了该内存始终驻留在物理内存中。
由于 GPU知道内存的物理地址，因为可以通过直接内存访问（Direct Memory Access）DMA技术在GPU和主机之间复制数据。
每当从可分页内存中执行复制操作时，复制速度将受限于PCIE的传输速度和系统前端总线速度相对较低的一方。
如果将所有malloc进行替换，则会更快的耗尽系统内存。
建议：仅对cudaMemcpy()调用中的源内存或者目标内存，才使用页锁定内存，并且在不需要它们时进行立即释放。&lt;/p&gt;

&lt;h2 id=&quot;测试程序&quot;&gt;测试程序&lt;/h2&gt;
&lt;p&gt;测试目标： 主要测试cudaMemcpy()在可分配内存和页锁定内存上的性能。分配一个GPU缓冲区以及一个大小相等的主机缓冲区，然后在这两个缓冲区之间执行复制操作。并设置CUDA事件进行精确的时间统计。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;float cuda_malloc_host_test(int size, bool up){

    int *a, *dev_a;
    float elapsedTime;

    cudaEvent_t start, stop;
    cudaEventCreate(&amp;amp;start);
    cudaEventCreate(&amp;amp;stop);

    cudaHostAlloc( (void**)&amp;amp;a, size * sizeof(*a), cudaHostAllocDefault);
    cudaMalloc((void**)&amp;amp;dev_a, size * sizeof(*dev_a));

    cudaEventRecord(start, 0);
    for(int i = 0; i &amp;lt; 100; i++){

        if(up)
            cudaMemcpy(dev_a, a, size * sizeof(*a), cudaMemcpyHostToDevice);
        else
            cudaMemcpy(a, dev_a, size * sizeof(*dev_a), cudaMemcpyDeviceToHost);
    }
    cudaEventRecord(stop, 0);
    cudaEventSynchronize(stop);

    cudaEventElapsedTime(&amp;amp;elapsedTime, start, stop);
    cudaFreeHost(a);
    cudaFree(dev_a);
    cudaEventDestroy(start);
    cudaEventDestroy(stop);

    return elapsedTime;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;以上可以看到，cudaHostAlloc()与malloc()分配的内存在使用方式上是相同的，不同的是最后一个参数cudaHostAllocDefault.我们可以通过修改该参数来修改cudaHostAlloc()的行为，并分配不同形式的固定主机内存。如果要释放内存，必须使用cudaFreeHost()。&lt;/p&gt;

&lt;h2 id=&quot;cuda流&quot;&gt;CUDA流&lt;/h2&gt;
&lt;p&gt;CUDA流表示一个GPU操作队列，并且队列上的操作将以指定的顺序执行。我们可以在流中添加一些操作。将这些操作添加的顺序就是它们的执行顺序。你可以将每个流视为GPU上的一个任务，并且这些任务可以并行执行。&lt;/p&gt;
&lt;h3 id=&quot;使用单个cuda流&quot;&gt;使用单个CUDA流&lt;/h3&gt;
&lt;p&gt;首先选择一个支持设备重叠功能的设备（Device Overlap）.支持设备重叠功能的GPU能够在执行一个CUDA核函数的同时,还能在设备与主机之间执行复制操作. 我们将使用多个流来实现这种计算与数据的传输的重叠,但首先来看如何创建和使用一个流.&lt;/p&gt;

&lt;p&gt;此次采用cudaMemcpyAsync()在GPU与主机之间复制数据.cudaMemcpy()函数是以同步的方式进行的,当函数返回时,复制操作已经完成.&lt;strong&gt;任何传递给cudaMemcpyAsync()的主机内存指针都必须已经通过cudaHostAlloc()分配好内存.也就是,你只能以异步的方式对页锁定内存进行复制操作.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;调用cudaStreamSynchronize()指定想要等待的流.&lt;/p&gt;

&lt;p&gt;cudaStreamDestroy()销毁流.&lt;/p&gt;

&lt;h2 id=&quot;使用多个cuda流&quot;&gt;使用多个CUDA流&lt;/h2&gt;
&lt;p&gt;在任何支持内存复制和核函数执行相互重叠的设备上,当使用多个流时,应用程序的性能都会整体提升.
建立两个流,stream0 和stream1两个流交替执行,实验测试性能确实提升.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  for(int i = 0; i &amp;lt; FULL_DATA_SIZE;i += N * 2){
        cudaMemcpyAsync(dev_a0, host_a + i, N* sizeof(int), cudaMemcpyHostToDevice, stream0);
        cudaMemcpyAsync(dev_b0, host_b + i, N* sizeof(int), cudaMemcpyHostToDevice, stream0);
        kernel&amp;lt;&amp;lt;&amp;lt;N/256, 256, 0, stream0&amp;gt;&amp;gt;&amp;gt;(dev_a0, dev_b0, dev_c0);
        cudaMemcpyAsync(host_c + i, dev_c0, N* sizeof(int), cudaMemcpyDeviceToHost, stream0);

        cudaMemcpyAsync(dev_a1, host_a + i + N, N* sizeof(int), cudaMemcpyHostToDevice, stream1);
        cudaMemcpyAsync(dev_b1, host_b + i +N, N* sizeof(int), cudaMemcpyHostToDevice, stream1);
        kernel&amp;lt;&amp;lt;&amp;lt;N/256, 256, 0, stream0&amp;gt;&amp;gt;&amp;gt;(dev_a1, dev_b1, dev_c1);
        cudaMemcpyAsync(host_c + i + N, dev_c1, N* sizeof(int), cudaMemcpyDeviceToHost, stream1);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;gpu的工作调度机制&quot;&gt;GPU的工作调度机制&lt;/h2&gt;
&lt;p&gt;需要确切理解GPU中流的执行机制,逻辑上来讲,不同流的运行是相互独立的,但是硬件上并没有流的概念,而是包含一个或多个引擎来执行内存的操作复制,以及执行核函数的引擎.&lt;strong&gt;书中图10.4,使用多个GPU流时的程序执行时间线,由于第0个流中将C复制回主机需要等待核函数执行完成才能进行,因此第1个流中虽然将a和b复制到GPU 的操作是完全独立的,但全被阻塞了.&lt;/strong&gt;因此我们需要知道将操作放入流中的顺序将影响着CUDA驱动程序调度这些操作以及执行的方式.&lt;/p&gt;
&lt;h2 id=&quot;高效地使用多个cuda流&quot;&gt;高效地使用多个CUDA流&lt;/h2&gt;
&lt;p&gt;要解决上述问题,在将操作放入流的队列时应采用宽度优先的方式,而非深度优先的方式.也就是说,我们并不是一次性添加第0个流的所有操作,而是两个流的操作交替进行添加.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  for(int i = 0; i &amp;lt; FULL_DATA_SIZE;i += N * 2){
        cudaMemcpyAsync(dev_a0, host_a + i, N* sizeof(int), cudaMemcpyHostToDevice, stream0);
        cudaMemcpyAsync(dev_b0, host_b + i, N* sizeof(int), cudaMemcpyHostToDevice, stream0);
        cudaMemcpyAsync(dev_a1, host_a + i + N, N* sizeof(int), cudaMemcpyHostToDevice, stream1);
        cudaMemcpyAsync(dev_b1, host_b + i +N, N* sizeof(int), cudaMemcpyHostToDevice, stream1);
        kernel&amp;lt;&amp;lt;&amp;lt;N/256, 256, 0, stream0&amp;gt;&amp;gt;&amp;gt;(dev_a0, dev_b0, dev_c0);
        kernel&amp;lt;&amp;lt;&amp;lt;N/256, 256, 0, stream0&amp;gt;&amp;gt;&amp;gt;(dev_a1, dev_b1, dev_c1);      
        cudaMemcpyAsync(host_c + i, dev_c0, N* sizeof(int), cudaMemcpyDeviceToHost, stream0);
        cudaMemcpyAsync(host_c + i + N, dev_c1, N* sizeof(int), cudaMemcpyDeviceToHost, stream1);

    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;执行顺图可看书中图10.5,也可比较上述两段代码.&lt;/p&gt;
&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;p&gt;本节介绍了在CUDA C程序中实现任务级的并行性.通过使用多个CUDA流,我们可以使GPU在执行核函数的同时,还能在主机和GPU之间执行复制操作.然而,采用这种方式时,需要注意里两个因素.首先,需要通过cudaHostAlloc()分配主机内存,因此接下来需要通过cudaMemcpyAsync()对内存复制操作进行排队,而异步复制操作需要在固定缓冲区执行.其次,我们需要知道添加操作到流中的顺序会对重叠情况产生影响.通常,应该采用宽度优先或者轮询的方式将工作分配到流.&lt;/p&gt;

</description>
        <pubDate>Sat, 22 Jun 2019 00:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/2019/06/22/CUDA-%E6%B5%81(Stream)/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2019/06/22/CUDA-%E6%B5%81(Stream)/</guid>
        
        <category>CUDA</category>
        
        
      </item>
    
      <item>
        <title>CUDA - 原子性</title>
        <description>&lt;h2 id=&quot;本章目标&quot;&gt;本章目标&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;了解NVIDIA GPU计算功能集&lt;/li&gt;
  &lt;li&gt;了解原子操作以及为什么需要它们&lt;/li&gt;
  &lt;li&gt;了解如何在CUDA C中执行带有原子操作的运算&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;nvidia-gpu计算功能集&quot;&gt;NVIDIA GPU计算功能集&lt;/h2&gt;
&lt;p&gt;功能集为1.2既支持共享内存原子操作又支持全局内存原子操作，功能集向下兼容，由于本书较老，目前功能集版本可查看最新设备列表。&lt;/p&gt;
&lt;h2 id=&quot;基于最小功能集的编译&quot;&gt;基于最小功能集的编译&lt;/h2&gt;
&lt;p&gt;可以指定只有在1.1或更高版本的计算功能集中才支持的编译优化。在编译时，只需增加命令选项:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nvcc -arch=sm_11
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;原子操作简介&quot;&gt;原子操作简介&lt;/h2&gt;
&lt;p&gt;将程序由单线程改为多线程时，如果多个线程需要对共享值进行读取或者写入，那么结果很有可能产生错误。
例如x++操作，此操作包括如下三个步骤：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;读取X的值&lt;/li&gt;
  &lt;li&gt;将读取的值加1&lt;/li&gt;
  &lt;li&gt;将递增结果写到X
现在有线程A和线程B都对X进行递增，理想情况如果X为7,则最终结果为9。
但是两个线程有可能交叉进行，造成错误。
在上述示例中，我们需要通过某种方式一次性执行完读取-修改-写入的操作，并且在执行过程中不被其它线程中断。具体来讲，除非已经完成了这三个操作，否则其他线程都不能读取或者写入X的值。由于这些操作的执行过程不能分解为更小的部分，因此我们将满足这种条件限制的操作称为&lt;strong&gt;原子操作&lt;/strong&gt;。CUDA C支持多种原子操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;示例程序中包括了CPU版与&lt;strong&gt;1 .使用全局内存原子操作的直方图核函数 2.使用共享内存原子和全局内存原子操作的直方图核函数&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1中与CPU性能对比时，可能会发现性能可能更糟。由于核函数只包含了非常少量的工作，很可能是全局内存上的原子操作导致了性能的降低。当数千个线程尝试访问少量的内存位置时，将导致大量竞争。为了确保递增操作的原子性，对相同位置的操作都将被硬件串行化。所以我们需要改进算法。&lt;/p&gt;

&lt;p&gt;2中在第一个阶段，每个并行线程块将计算它所处理数据的直方图。由于每个线程块在执行这个操作时是相互独立的，因此可以在共享内存中计算这些直方图。由于现在只有256个线程在256个地址上发生竞争，这将极大的减少在使用全局内存时在数千个线程之间发生竞争的情况。&lt;/p&gt;

&lt;p&gt;示例程序请看https://github.com/mengzhangjian/cuda-learning-example 中CH09。&lt;/p&gt;

</description>
        <pubDate>Wed, 19 Jun 2019 00:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/2019/06/19/CUDA-%E5%8E%9F%E5%AD%90%E6%80%A7/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2019/06/19/CUDA-%E5%8E%9F%E5%AD%90%E6%80%A7/</guid>
        
        <category>CUDA</category>
        
        
      </item>
    
      <item>
        <title>CUDA - 纹理内存</title>
        <description>&lt;h2 id=&quot;本章目标&quot;&gt;本章目标&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;使用一维纹理内存&lt;/li&gt;
  &lt;li&gt;使用二维纹理内存&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;纹理内存简介&quot;&gt;纹理内存简介&lt;/h2&gt;
&lt;p&gt;纹理内存是另一种只读内存，缓存在芯片上，可减少内存请求并提供高效带宽。纹理内存是专门为在内存访问模式中存在大量空间局部性的图形应用程序而设计。在某个程序中，这意味着一个线程读取的位置与邻近线程读取的位置很近。&lt;/p&gt;

&lt;h2 id=&quot;热传导模拟代码&quot;&gt;热传导模拟代码&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &quot;cuda.h&quot;
#include &quot;../common/book.h&quot;
#include &quot;../common/image.h&quot;

#define DIM 1024
#define PI 3.1415926535897932f
#define MAX_TEMP 1.0f
#define MIN_TEMP 0.0001f
#define SPEED   0.25f

// these exist on the GPU side
texture&amp;lt;float&amp;gt;  texConstSrc;
texture&amp;lt;float&amp;gt;  texIn;
texture&amp;lt;float&amp;gt;  texOut;



// this kernel takes in a 2-d array of floats
// it updates the value-of-interest by a scaled value based
// on itself and its nearest neighbors
__global__ void blend_kernel( float *dst,
                              bool dstOut ) {
    // map from threadIdx/BlockIdx to pixel position
    int x = threadIdx.x + blockIdx.x * blockDim.x;
    int y = threadIdx.y + blockIdx.y * blockDim.y;
    int offset = x + y * blockDim.x * gridDim.x;

    int left = offset - 1;
    int right = offset + 1;
    if (x == 0)   left++;
    if (x == DIM-1) right--;

    int top = offset - DIM;
    int bottom = offset + DIM;
    if (y == 0)   top += DIM;
    if (y == DIM-1) bottom -= DIM;

    float   t, l, c, r, b;
    if (dstOut) {
        t = tex1Dfetch(texIn,top);
        l = tex1Dfetch(texIn,left);
        c = tex1Dfetch(texIn,offset);
        r = tex1Dfetch(texIn,right);
        b = tex1Dfetch(texIn,bottom);

    } else {
        t = tex1Dfetch(texOut,top);
        l = tex1Dfetch(texOut,left);
        c = tex1Dfetch(texOut,offset);
        r = tex1Dfetch(texOut,right);
        b = tex1Dfetch(texOut,bottom);
    }
    dst[offset] = c + SPEED * (t + b + r + l - 4 * c);
}

// NOTE - texOffsetConstSrc could either be passed as a
// parameter to this function, or passed in __constant__ memory
// if we declared it as a global above, it would be
// a parameter here:
// __global__ void copy_const_kernel( float *iptr,
//                                    size_t texOffset )
__global__ void copy_const_kernel( float *iptr ) {
    // map from threadIdx/BlockIdx to pixel position
    int x = threadIdx.x + blockIdx.x * blockDim.x;
    int y = threadIdx.y + blockIdx.y * blockDim.y;
    int offset = x + y * blockDim.x * gridDim.x;

    float c = tex1Dfetch(texConstSrc,offset);
    if (c != 0)
        iptr[offset] = c;
}

// globals needed by the update routine
struct DataBlock {
    unsigned char   *output_bitmap;
    float           *dev_inSrc;
    float           *dev_outSrc;
    float           *dev_constSrc;
    IMAGE  *bitmap;

    cudaEvent_t     start, stop;
    float           totalTime;
    float           frames;
};

void anim_gpu( DataBlock *d, int ticks ) {

}

// clean up memory allocated on the GPU
void cleanup( DataBlock *d ) {
    cudaUnbindTexture( texIn );
    cudaUnbindTexture( texOut );
    cudaUnbindTexture( texConstSrc );
    HANDLE_ERROR( cudaFree( d-&amp;gt;dev_inSrc ) );
    HANDLE_ERROR( cudaFree( d-&amp;gt;dev_outSrc ) );
    HANDLE_ERROR( cudaFree( d-&amp;gt;dev_constSrc ) );

    HANDLE_ERROR( cudaEventDestroy( d-&amp;gt;start ) );
    HANDLE_ERROR( cudaEventDestroy( d-&amp;gt;stop ) );
}


int main( void ) {
    DataBlock   data;
    IMAGE bitmap_image( DIM, DIM );
    data.bitmap = &amp;amp;bitmap_image;
    data.totalTime = 0;
    data.frames = 0;
    HANDLE_ERROR( cudaEventCreate( &amp;amp;data.start ) );
    HANDLE_ERROR( cudaEventCreate( &amp;amp;data.stop ) );

    int imageSize = bitmap_image.image_size();

    HANDLE_ERROR( cudaMalloc( (void**)&amp;amp;data.output_bitmap,
                              imageSize ) );

    // assume float == 4 chars in size (ie rgba)
    HANDLE_ERROR( cudaMalloc( (void**)&amp;amp;data.dev_inSrc,
                              imageSize ) );
    HANDLE_ERROR( cudaMalloc( (void**)&amp;amp;data.dev_outSrc,
                              imageSize ) );
    HANDLE_ERROR( cudaMalloc( (void**)&amp;amp;data.dev_constSrc,
                              imageSize ) );

    HANDLE_ERROR( cudaBindTexture( NULL, texConstSrc,
                                   data.dev_constSrc,
                                   imageSize ) );

    HANDLE_ERROR( cudaBindTexture( NULL, texIn,
                                   data.dev_inSrc,
                                   imageSize ) );

    HANDLE_ERROR( cudaBindTexture( NULL, texOut,
                                   data.dev_outSrc,
                                   imageSize ) );

    // intialize the constant data
    float *temp = (float*)malloc( imageSize );
    for (int i=0; i&amp;lt;DIM*DIM; i++) {
        temp[i] = 0;
        int x = i % DIM;
        int y = i / DIM;
        if ((x&amp;gt;300) &amp;amp;&amp;amp; (x&amp;lt;600) &amp;amp;&amp;amp; (y&amp;gt;310) &amp;amp;&amp;amp; (y&amp;lt;601))
            temp[i] = MAX_TEMP;
    }
    temp[DIM*100+100] = (MAX_TEMP + MIN_TEMP)/2;
    temp[DIM*700+100] = MIN_TEMP;
    temp[DIM*300+300] = MIN_TEMP;
    temp[DIM*200+700] = MIN_TEMP;
    for (int y=800; y&amp;lt;900; y++) {
        for (int x=400; x&amp;lt;500; x++) {
            temp[x+y*DIM] = MIN_TEMP;
        }
    }
    HANDLE_ERROR( cudaMemcpy( data.dev_constSrc, temp,
                              imageSize,
                              cudaMemcpyHostToDevice ) );

    // initialize the input data
    for (int y=800; y&amp;lt;DIM; y++) {
        for (int x=0; x&amp;lt;200; x++) {
            temp[x+y*DIM] = MAX_TEMP;
        }
    }
    HANDLE_ERROR( cudaMemcpy( data.dev_inSrc, temp,
                              imageSize,
                              cudaMemcpyHostToDevice ) );
    free( temp );

    int ticks=0;
    bitmap_image.show_image(30);
    while(1)
    {
        HANDLE_ERROR( cudaEventRecord( data.start, 0 ) );
        dim3    blocks(DIM/16,DIM/16);
        dim3    threads(16,16);
        IMAGE  *bitmap = data.bitmap;

        // since tex is global and bound, we have to use a flag to
        // select which is in/out per iteration
        volatile bool dstOut = true;
        for (int i=0; i&amp;lt;90; i++)
        {
            float   *in, *out;
            if (dstOut)
            {
                in  = data.dev_inSrc;
                out = data.dev_outSrc;
            }
            else
            {
                out = data.dev_inSrc;
                in  = data.dev_outSrc;
            }
            copy_const_kernel&amp;lt;&amp;lt;&amp;lt;blocks,threads&amp;gt;&amp;gt;&amp;gt;( in );
            blend_kernel&amp;lt;&amp;lt;&amp;lt;blocks,threads&amp;gt;&amp;gt;&amp;gt;( out, dstOut );
            dstOut = !dstOut;
        }
        float_to_color&amp;lt;&amp;lt;&amp;lt;blocks,threads&amp;gt;&amp;gt;&amp;gt;( data.output_bitmap,
                data.dev_inSrc );

        HANDLE_ERROR( cudaMemcpy( bitmap-&amp;gt;get_ptr(),
                                  data.output_bitmap,
                                  bitmap-&amp;gt;image_size(),
                                  cudaMemcpyDeviceToHost ) );

        HANDLE_ERROR( cudaEventRecord( data.stop, 0 ) );
        HANDLE_ERROR( cudaEventSynchronize( data.stop ) );
        float   elapsedTime;
        HANDLE_ERROR( cudaEventElapsedTime( &amp;amp;elapsedTime,
                                            data.start, data.stop ) );
        data.totalTime += elapsedTime;
        ++data.frames;
        printf( &quot;Average Time per frame:  %3.1f ms\n&quot;,
                data.totalTime/data.frames  );

        ticks++;
        char key = bitmap_image.show_image(30);
        if(key==27)
        {
            break;
        }
    }

    cleanup(&amp;amp;data);

    return 0;
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;掌握texture类型， cudaBindTexture, tex1Dfetch()与cudaUnbindTexture()
 本节有难度，待继续更新。&lt;/p&gt;

&lt;h2 id=&quot;使用二维纹理内存&quot;&gt;使用二维纹理内存&lt;/h2&gt;

&lt;p&gt;默认纹理引用都是一维的，因此增加代表维数的参数，这表示声明的是一个二维纹理引用&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;texture&amp;lt;float, 2&amp;gt; texConstSrc;
texture&amp;lt;float, 2&amp;gt; texIn;
texture&amp;lt;float, 2&amp;gt; texOut;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;需要将tex1Dfetch()调用改为tex2D()调用，不需要再通过线性化offset计算偏移，可以直接通过X， Y访问纹理，也不用担心溢出问题。如果 x或者y小于0, tex2D()将返回0处的值。
同时绑定二维纹理cudaBindTexture2D(),CUDA要求提供一个cudaChannelFormatDesc。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cudaChannelFormatDesc desc = cudaCreateChannelDesc&amp;lt;float&amp;gt;();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;热传导二维纹理代码&quot;&gt;热传导二维纹理代码&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &quot;cuda.h&quot;
#include &quot;../common/book.h&quot;
#include &quot;../common/image.h&quot;

#define DIM 1024
#define PI 3.1415926535897932f
#define MAX_TEMP 1.0f
#define MIN_TEMP 0.0001f
#define SPEED   0.25f

// these exist on the GPU side
texture&amp;lt;float, 2&amp;gt;  texConstSrc;
texture&amp;lt;float, 2&amp;gt;  texIn;
texture&amp;lt;float, 2&amp;gt;  texOut;



// this kernel takes in a 2-d array of floats
// it updates the value-of-interest by a scaled value based
// on itself and its nearest neighbors
__global__ void blend_kernel( float *dst,
                              bool dstOut ) {
    // map from threadIdx/BlockIdx to pixel position
    int x = threadIdx.x + blockIdx.x * blockDim.x;
    int y = threadIdx.y + blockIdx.y * blockDim.y;
    int offset = x + y * blockDim.x * gridDim.x;


    float   t, l, c, r, b;
    if (dstOut) {
        t = tex2D(texIn, x, y - 1);
        l = tex2D(texIn, x -1, y);
        c = tex2D(texIn, x, y);
        r = tex2D(texIn, x + 1, y);
        b = tex2D(texIn, x, y + 1);

    } else {
        t = tex2D(texOut, x, y - 1);
        l = tex2D(texOut, x -1, y);
        c = tex2D(texOut, x, y);
        r = tex2D(texOut, x + 1, y);
        b = tex2D(texOut, x, y + 1);
    }
    dst[offset] = c + SPEED * (t + b + r + l - 4 * c);
}

// NOTE - texOffsetConstSrc could either be passed as a
// parameter to this function, or passed in __constant__ memory
// if we declared it as a global above, it would be
// a parameter here:
// __global__ void copy_const_kernel( float *iptr,
//                                    size_t texOffset )
__global__ void copy_const_kernel( float *iptr ) {
    // map from threadIdx/BlockIdx to pixel position
    int x = threadIdx.x + blockIdx.x * blockDim.x;
    int y = threadIdx.y + blockIdx.y * blockDim.y;
    int offset = x + y * blockDim.x * gridDim.x;

    float c = tex2D(texConstSrc, x, y);
    if (c != 0)
        iptr[offset] = c;
}

// globals needed by the update routine
struct DataBlock {
    unsigned char   *output_bitmap;
    float           *dev_inSrc;
    float           *dev_outSrc;
    float           *dev_constSrc;
    IMAGE  *bitmap;

    cudaEvent_t     start, stop;
    float           totalTime;
    float           frames;
};

void anim_gpu( DataBlock *d, int ticks ) {

}

// clean up memory allocated on the GPU
void cleanup( DataBlock *d ) {
    cudaUnbindTexture( texIn );
    cudaUnbindTexture( texOut );
    cudaUnbindTexture( texConstSrc );
    HANDLE_ERROR( cudaFree( d-&amp;gt;dev_inSrc ) );
    HANDLE_ERROR( cudaFree( d-&amp;gt;dev_outSrc ) );
    HANDLE_ERROR( cudaFree( d-&amp;gt;dev_constSrc ) );

    HANDLE_ERROR( cudaEventDestroy( d-&amp;gt;start ) );
    HANDLE_ERROR( cudaEventDestroy( d-&amp;gt;stop ) );
}


int main( void ) {
    DataBlock   data;
    IMAGE bitmap_image( DIM, DIM );
    data.bitmap = &amp;amp;bitmap_image;
    data.totalTime = 0;
    data.frames = 0;
    HANDLE_ERROR( cudaEventCreate( &amp;amp;data.start ) );
    HANDLE_ERROR( cudaEventCreate( &amp;amp;data.stop ) );

    int imageSize = bitmap_image.image_size();

    HANDLE_ERROR( cudaMalloc( (void**)&amp;amp;data.output_bitmap,
                              imageSize ) );

    // assume float == 4 chars in size (ie rgba)
    HANDLE_ERROR( cudaMalloc( (void**)&amp;amp;data.dev_inSrc,
                              imageSize ) );
    HANDLE_ERROR( cudaMalloc( (void**)&amp;amp;data.dev_outSrc,
                              imageSize ) );
    HANDLE_ERROR( cudaMalloc( (void**)&amp;amp;data.dev_constSrc,
                              imageSize ) );

    cudaChannelFormatDesc desc = cudaCreateChannelDesc&amp;lt;float&amp;gt;();
    HANDLE_ERROR( cudaBindTexture2D( NULL, texConstSrc,
                                   data.dev_constSrc,
                                   desc, DIM, DIM, sizeof(float) * DIM) );

    HANDLE_ERROR( cudaBindTexture2D( NULL, texIn,
                                   data.dev_inSrc,
                                   desc, DIM, DIM, sizeof(float) * DIM) );

    HANDLE_ERROR( cudaBindTexture2D( NULL, texOut,
                                   data.dev_outSrc,
                                   desc, DIM, DIM, sizeof(float) * DIM) );

    // intialize the constant data
    float *temp = (float*)malloc( imageSize );
    for (int i=0; i&amp;lt;DIM*DIM; i++) {
        temp[i] = 0;
        int x = i % DIM;
        int y = i / DIM;
        if ((x&amp;gt;300) &amp;amp;&amp;amp; (x&amp;lt;600) &amp;amp;&amp;amp; (y&amp;gt;310) &amp;amp;&amp;amp; (y&amp;lt;601))
            temp[i] = MAX_TEMP;
    }
    temp[DIM*100+100] = (MAX_TEMP + MIN_TEMP)/2;
    temp[DIM*700+100] = MIN_TEMP;
    temp[DIM*300+300] = MIN_TEMP;
    temp[DIM*200+700] = MIN_TEMP;
    for (int y=800; y&amp;lt;900; y++) {
        for (int x=400; x&amp;lt;500; x++) {
            temp[x+y*DIM] = MIN_TEMP;
        }
    }
    HANDLE_ERROR( cudaMemcpy( data.dev_constSrc, temp,
                              imageSize,
                              cudaMemcpyHostToDevice ) );

    // initialize the input data
    for (int y=800; y&amp;lt;DIM; y++) {
        for (int x=0; x&amp;lt;200; x++) {
            temp[x+y*DIM] = MAX_TEMP;
        }
    }
    HANDLE_ERROR( cudaMemcpy( data.dev_inSrc, temp,
                              imageSize,
                              cudaMemcpyHostToDevice ) );
    free( temp );

    int ticks=0;
    bitmap_image.show_image(30);
    while(1)
    {
        HANDLE_ERROR( cudaEventRecord( data.start, 0 ) );
        dim3    blocks(DIM/16,DIM/16);
        dim3    threads(16,16);
        IMAGE  *bitmap = data.bitmap;

        // since tex is global and bound, we have to use a flag to
        // select which is in/out per iteration
        volatile bool dstOut = true;
        for (int i=0; i&amp;lt;90; i++)
        {
            float   *in, *out;
            if (dstOut)
            {
                in  = data.dev_inSrc;
                out = data.dev_outSrc;
            }
            else
            {
                out = data.dev_inSrc;
                in  = data.dev_outSrc;
            }
            copy_const_kernel&amp;lt;&amp;lt;&amp;lt;blocks,threads&amp;gt;&amp;gt;&amp;gt;( in );
            blend_kernel&amp;lt;&amp;lt;&amp;lt;blocks,threads&amp;gt;&amp;gt;&amp;gt;( out, dstOut );
            dstOut = !dstOut;
        }
        float_to_color&amp;lt;&amp;lt;&amp;lt;blocks,threads&amp;gt;&amp;gt;&amp;gt;( data.output_bitmap,
                data.dev_inSrc );

        HANDLE_ERROR( cudaMemcpy( bitmap-&amp;gt;get_ptr(),
                                  data.output_bitmap,
                                  bitmap-&amp;gt;image_size(),
                                  cudaMemcpyDeviceToHost ) );

        HANDLE_ERROR( cudaEventRecord( data.stop, 0 ) );
        HANDLE_ERROR( cudaEventSynchronize( data.stop ) );
        float   elapsedTime;
        HANDLE_ERROR( cudaEventElapsedTime( &amp;amp;elapsedTime,
                                            data.start, data.stop ) );
        data.totalTime += elapsedTime;
        ++data.frames;
        printf( &quot;Average Time per frame:  %3.1f ms\n&quot;,
                data.totalTime/data.frames  );

        ticks++;
        char key = bitmap_image.show_image(30);
        if(key==27)
        {
            break;
        }
    }

    cleanup(&amp;amp;data);

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Tue, 18 Jun 2019 00:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/2019/06/18/CUDA-%E7%BA%B9%E7%90%86%E5%86%85%E5%AD%98/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2019/06/18/CUDA-%E7%BA%B9%E7%90%86%E5%86%85%E5%AD%98/</guid>
        
        <category>CUDA</category>
        
        
      </item>
    
  </channel>
</rss>
