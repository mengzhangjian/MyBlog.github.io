---
layout:     post
title:      大语言模型微调实践记录
subtitle:   是大模型呀
date:       2023-09-08
author:     BY    J
header-img: img/post-bg-mma-4.jpg
catalog: true
tags:
    - 大模型
---
# 大语言模型微调实践记录

### 背景

根据目前公开的训练范式，大模型微调主要经过三个步骤的训练: 

- 万亿Token预训练
- SFT指令对齐
- RLHF阶段

目前开源的模型大部分为基座大模型与对齐后的chat模型，大体对应了两个阶段中的1与2阶段。本篇将记录根据开源的基座大模型进行SFT微调的实战记录。

基座大模型是不具备对话能力的，所以回答会出现牛头不对马嘴的情况。所以需要我们针对具体的对话数据进行SFT微调，使之具备初步的对话能力。

3月初时，笔者当时根据羊驼项目进行过Llama模型的初步微调，但当时受客观条件限制——训练时间过长与数据集准备不充分、模型了解不多，导致训练完毕后，生成部分会出现token重复，回答截断等问题。所以微调阶段一度放弃。

时间来到9月份，一是认为微调不出具备良好效果的大模型则无法踩坑，也不会有太大成长。二是毕竟这是大势所趋，作为算法工程师应是必备技能。另外，随着陆续开源，可借鉴的资料也很多，所以再度挤时间投入这部分的实验验证。

模型训练大致需要做好三个准备工作：

- 数据集准备
- 模型训练代码
- 资源环境配置、评估工作

### 数据部分

数据集部分，对于超大数据量数据，通常需要做数据清洗与聚类去重操作，这里将常见的清洗类型与聚类方法进行列举：

- 清洗类型节选自百度: **[文心一言数据清洗使用说明](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Mliu6hgzo)**
- 去重操作: **[BigCode 背后的大规模数据去重](https://huggingface.co/blog/zh/dedup)**

这里，我们不涉及数据清洗与数据去重操作，在已知的高质量指令数据中选取复旦大学MOSS团队开源的中英文多轮对话数据。

### 模型部分

本次微调基于开源Firefly项目（代码友好型，组织结构易于理解）。基座模型选取Baichuan-7B与

Baichuan2-7B进行对比。模型的具体细节不再赘述。

在实际测试过程中，基座模型是不具备对话能力的，所以需要针对基座模型进行SFT的微调。这里之前我一直有误解。这里值得一提的是在多轮对话训练过程中，Firefly损失函数处稍有改进，具体讲解见[**此处**](https://yuansearch.com/2023/09/02/%E5%A6%82%E4%BD%95%E5%85%85%E5%88%86%E9%AB%98%E6%95%88%E8%AE%AD%E7%BB%83%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B/)。

### 训练部分

**日志记录**

训练过程各相关曲线的观察，这里未采用原项目中的tensorboard，而是选用的[**wandb**](https://wandb.ai/site)。

起初实验验证部分选取了MOSS前10万条数据，epoch=1，在两个A6000大约耗时22小时进行微调，结果发现模型已实现了基本的指令对齐，而且并未出现回答token重复等现象出现，这已验证了Firefly项目的良好微调特性。

Moss前10万，Epoch=1

效果图

![https://res.cloudinary.com/dsn0i1fsm/image/upload/v1694168738/firefly_o5mcdi.png](https://res.cloudinary.com/dsn0i1fsm/image/upload/v1694168738/firefly_o5mcdi.png)

目前正在进行基于baichuan-7B与Baichuan2-7B在MOSS上的全量数据微调，相关结果等数据出现后会再公布。

**Update**: 

**2023-09-10**

**baichuan-7B**  

数据集: Moss

微调日志: 机器 4卡- A100

```python
【2023-09-10 10:18:45】***** train metrics *****
1350
【2023-09-10 10:18:45】 epoch = 1.0
1351
【2023-09-10 10:18:45】 train_loss = 0.7111
1352
【2023-09-10 10:18:45】 train_runtime = 1 day, 20:04:26.08
1353
【2023-09-10 10:18:45】 train_samples_per_second = 6.772
1354
【2023-09-10 10:18:45】 train_steps_per_second = 0.053
1355
【2023-09-10 10:18:45】wandb: Waiting for W&B process to finish... (success).

【2023-09-10 10:18:46】wandb: Run summary:
1369
【2023-09-10 10:18:46】wandb: train/epoch 1.0
1370
【2023-09-10 10:18:46】wandb: train/global_step 8395
1371
【2023-09-10 10:18:46】wandb: train/learning_rate 0.0002
1372
【2023-09-10 10:18:46】wandb: train/loss 0.6591
1373
【2023-09-10 10:18:46】wandb: train/total_flos 2.4055027002056376e+19
1374
【2023-09-10 10:18:46】wandb: train/train_loss 0.71107
1375
【2023-09-10 10:18:46】wandb: train/train_runtime 158666.0883
1376
【2023-09-10 10:18:46】wandb: train/train_samples_per_second 6.772
1377
【2023-09-10 10:18:46】wandb: train/train_steps_per_second 0.053
```

**多轮对话效果图**

| User | hello |
| --- | --- |
| Moss | Hello! How can I assist you today? |
| User | 你谁 会说中文吗 |
| Moss | 是的，我可以使用中文与您交流。 |
| User | 北京连续下了两天雨了，已经入秋，你能作首宋词描述一下吗 |
| Moss | 当然可以，以下是我为您创作的一首宋词：<br />《秋雨》<br />秋雨绵绵，秋风萧瑟，<br />梧桐叶落，满地黄。<br />秋雨绵绵，秋风萧瑟，<br />万物凋零，人心思归。<br />秋雨绵绵，秋风萧瑟，<br />梧桐叶落，满地黄。<br />秋雨绵绵，秋风萧瑟，<br />万物凋零，人心思归。<br /> |
| User | 你能背诵李白的将进酒吗 |
| Moss | 当然可以，以下是我为您背诵的李白的《将进酒》：<br />君不见黄河之水天上来，奔流到海不复回。<br />君不见高堂明镜悲白发，朝如青丝暮成雪。<br />人生得意须尽欢，莫使金樽空对月。<br />天生我材必有用，千金散尽还复来。<br />烹羊宰牛且为乐，会须一饮三百杯。<br />岑夫子，丹丘生，将进酒，杯莫停。<br />与君歌一曲，请君为我倾耳听。<br />钟鼓馔玉不足贵，但愿长醉不复醒。<br />古来圣贤皆寂寞，惟有饮者留其名。<br />陈王昔时宴平乐，斗酒十千恣欢谑。<br />主人何为言少钱，径须沽取对君酌。<br />五花马，千金裘，呼儿将出换美酒，与尔同销万古愁。 |
| User | 可以把最后一句翻译成英文吗 |
| Moss | Sure, the last line of the poem is:<br />"五花马，千金裘，呼儿将出换美酒，与尔同销万古愁。"<br />Translation:<br />"Five-colored horse, thousand-gold coat, call my son to exchange for wine, and you can help me forget the world for thousands of years.” |
| User | 你做的很不错呢 |
| Moss | 谢谢夸奖！我会继续努力的。 |
| User | 你认识黑塞吗 |
| Moss | 黑塞是德国作家，他的作品被翻译成多种语言，包括中文。他的代表作包括《荒原狼》、《流浪者之歌》等。 |
| User | 荒原狼讲的 什么啊 |
| Moss | 《荒原狼》是黑塞的代表作之一，讲述了一个孤独的男子在荒原上寻找自我和意义的故事。主人公哈里在现实生活中感到孤独和无助，于是他开始漫游荒原，寻找自我和意义。在荒原上，他遇到了许多人和事物，包括一个神秘的流浪者，一个神秘的女子，以及一只狼。在与这些人和事物的接触中，哈里逐渐找到了自我和意义，最终回到了现实生活中。 |
| User | 我刚要开始读它，有什么建议吗 |
| Moss | 如果你想开始读《荒原狼》，我建议你先了解一些背景知识。黑塞的作品通常涉及到哲学、宗教、历史等多个领域，需要有一定的阅读基础。此外，黑塞的作品通常比较深刻，需要有一定的思考和理解能力。如果你对这些方面不太熟悉，可以先阅读一些相关的介绍和导读，或者选择一些比较易读的作品。最后，阅读黑塞的作品需要耐心和时间，不要急于求成，慢慢地品味其中的深意和美感。 |
| User | 好的，谢谢了，周末愉快 |
| Moss | 很高兴能够帮到你，祝你周末愉快！如果你有任何其他问题，随时都可以问我。 |

从几个测试样例看，效果还是蛮不错的，唯独宋词部分重复输出，综合细评后续给出。

公司目前已开放了ModelHub平台，但是我并未采用此方式进行微调训练。

主要采用了两种方式：

1. 实验验证部分，基于离线容器，通过创建python虚拟容器进行环境配置。
2. 全量数据部分，目前正通过鲁班平台→离线任务→开发任务 进行微调，但是该方法存在稳定性的问题，偶尔会因为显存问题训练过程被中断。

下面列举详细配置及参数

 1.  Firefly项目: 

 项目介绍页会列举torch版本的区别，可全部采用2.0以上版本, python版本3.8.10。

```python
accelerate==0.21.0
peft==0.4.0
bitsandbytes==0.41.0
loguru==0.7.0
numpy==1.21.4
pandas==1.2.5
tqdm==4.62.3
deepspeed==0.9.5
tensorboard
sentencepiece
transformers_stream_generator
tiktoken
einops
Xformers
torch==2.0.1
scipy
wandb
```

transformers需要原包安装

2.模型训练参数

```python
{
    "output_dir": "output/firefly-baichuan-7b-1000000",
    "model_name_or_path": "baichuan-7B",
    "cache_dir": "cache_model",
    "train_file": "./data/moss-003-sft-data.jsonl",
    "num_train_epochs": 1,
    "per_device_train_batch_size": 16,
    "gradient_accumulation_steps": 2,
    "learning_rate": 2e-4,
    "max_seq_length": 1024,
    "logging_steps": 50,
    "save_steps": 100,
    "save_total_limit": 1,
    "lr_scheduler_type": "constant_with_warmup",
    "warmup_steps": 1000,
    "lora_rank": 64,
    "lora_alpha": 16,
    "lora_dropout": 0.05,
    "bits": 4,

    "gradient_checkpointing": true,
    "disable_tqdm": false,
    "optim": "paged_adamw_32bit",
    "seed": 42,
    "fp16": true,
    "report_to": "wandb",
    "dataloader_num_workers": 0,
    "save_strategy": "steps",
    "weight_decay": 0,
    "max_grad_norm": 0.3,
    "remove_unused_columns": false,
    "do_eval": false,
    "evaluation_strategy": "no"
}
```

**模型评估**

该部分待全量模型训练完毕后再进行补充

### TO-DO

- [ ]  更改Dataloader为动态加载文件方式，以适应超大规模数据
- [ ]  添加模型评估方法
- [ ]  添加交互界面以供测试，提供http调用

**FAQ**:

```python
1. 训练提示bitsandbytes报错？

 Transformer版本一定要用原库安装，一般安装命令

 pip install git+https:*//github.com/huggingface/transformers.git
 不过由于公司平台网络限制原因，建议直接下载git包后，通过pip install -e .
 的方式进行安装

2. 如果发生生成token重复怎么办？
 生成token重复通常可能因训练不充分，或数据集质量太差，另外也可通过生成参数去控制，具体可见[**此处**](https://yuansearch.com/2023/08/14/%E8%A7%A3%E7%A0%81%E7%9A%84%E7%94%9F%E6%88%90%E5%A4%9A%E6%A0%B7%E6%80%A7/)
```

以上为本周进展，本篇将根据具体实验进展不断更新，以上部分观点仅为个人见解，如有错误，望批评指正。