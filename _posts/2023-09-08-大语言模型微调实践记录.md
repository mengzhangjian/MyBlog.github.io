---
layout:     post
title:      大语言模型微调实践记录
subtitle:   是大模型呀
date:       2023-09-08
author:     BY    J
header-img: img/post-bg-mma-4.jpg
catalog: true
tags:
    - 大模型
---
# 大语言模型微调实践记录

### 背景

根据目前公开的训练范式，大模型微调主要经过三个步骤的训练: 

- 万亿Token预训练
- SFT指令对齐
- RLHF阶段

目前开源的模型大部分为基座大模型与对齐后的chat模型，大体对应了两个阶段中的1与2阶段。本篇将记录根据开源的基座大模型进行SFT微调的实战记录。

基座大模型是不具备对话能力的，所以回答会出现牛头不对马嘴的情况。所以需要我们针对具体的对话数据进行SFT微调，使之具备初步的对话能力。

3月初时，笔者当时根据羊驼项目进行过Llama模型的初步微调，但当时受客观条件限制——训练时间过长与数据集准备不充分、模型了解不多，导致训练完毕后，生成部分会出现token重复，回答截断等问题。所以微调阶段一度放弃。

时间来到9月份，一是认为微调不出具备良好效果的大模型则无法踩坑，也不会有太大成长。二是毕竟这是大势所趋，作为算法工程师应是必备技能。另外，随着陆续开源，可借鉴的资料也很多，所以再度挤时间投入这部分的实验验证。

模型训练大致需要做好三个准备工作：

- 数据集准备
- 模型训练代码
- 资源环境配置、评估工作

### 数据部分

数据集部分，对于超大数据量数据，通常需要做数据清洗与聚类去重操作，这里将常见的清洗类型与聚类方法进行列举：

- 清洗类型节选自百度: **[文心一言数据清洗使用说明](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Mliu6hgzo)**
- 去重操作: **[BigCode 背后的大规模数据去重](https://huggingface.co/blog/zh/dedup)**

这里，我们不涉及数据清洗与数据去重操作，在已知的高质量指令数据中选取复旦大学MOSS团队开源的中英文多轮对话数据。

### 模型部分

本次微调基于开源Firefly项目（代码友好型，组织结构易于理解）。基座模型选取Baichuan-7B与

Baichuan2-7B进行对比。模型的具体细节不再赘述。

在实际测试过程中，基座模型是不具备对话能力的，所以需要针对基座模型进行SFT的微调。这里之前我一直有误解。这里值得一提的是在多轮对话训练过程中，Firefly损失函数处稍有改进，具体讲解见[**此处**](https://yuansearch.com/2023/09/02/%E5%A6%82%E4%BD%95%E5%85%85%E5%88%86%E9%AB%98%E6%95%88%E8%AE%AD%E7%BB%83%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B/)。

### 训练部分

**日志记录**

训练过程各相关曲线的观察，这里未采用原项目中的tensorboard，而是选用的[**wandb**](https://wandb.ai/site)。

起初实验验证部分选取了MOSS前10万条数据，epoch=1，在两个A6000大约耗时22小时进行微调，结果发现模型已实现了基本的指令对齐，而且并未出现回答token重复等现象出现，这已验证了Firefly项目的良好微调特性。

Moss前10万，Epoch=1

效果图

![https://res.cloudinary.com/dsn0i1fsm/image/upload/v1694168738/firefly_o5mcdi.png](https://res.cloudinary.com/dsn0i1fsm/image/upload/v1694168738/firefly_o5mcdi.png)

目前正在进行基于baichuan-7B与Baichuan2-7B在MOSS上的全量数据微调，相关结果等数据出现后会再公布。

公司目前已开放了ModelHub平台，但是我并未采用此方式进行微调训练。

主要采用了两种方式：

1. 实验验证部分，基于离线容器，通过创建python虚拟容器进行环境配置。
2. 全量数据部分，目前正通过鲁班平台→离线任务→开发任务 进行微调，但是该方法存在稳定性的问题，偶尔会因为显存问题训练过程被中断。

下面列举详细配置及参数

 1.  Firefly项目: 

 项目介绍页会列举torch版本的区别，可全部采用2.0以上版本, python版本3.8.10。

```python
accelerate==0.21.0
peft==0.4.0
bitsandbytes==0.41.0
loguru==0.7.0
numpy==1.21.4
pandas==1.2.5
tqdm==4.62.3
deepspeed==0.9.5
tensorboard
sentencepiece
transformers_stream_generator
tiktoken
einops
Xformers
torch==2.0.1
scipy
wandb
```

2.模型训练参数

```python
{
    "output_dir": "output/firefly-baichuan-7b-1000000",
    "model_name_or_path": "baichuan-7B",
    "cache_dir": "cache_model",
    "train_file": "./data/moss-003-sft-data.jsonl",
    "num_train_epochs": 1,
    "per_device_train_batch_size": 16,
    "gradient_accumulation_steps": 2,
    "learning_rate": 2e-4,
    "max_seq_length": 1024,
    "logging_steps": 50,
    "save_steps": 100,
    "save_total_limit": 1,
    "lr_scheduler_type": "constant_with_warmup",
    "warmup_steps": 1000,
    "lora_rank": 64,
    "lora_alpha": 16,
    "lora_dropout": 0.05,
    "bits": 4,

    "gradient_checkpointing": true,
    "disable_tqdm": false,
    "optim": "paged_adamw_32bit",
    "seed": 42,
    "fp16": true,
    "report_to": "wandb",
    "dataloader_num_workers": 0,
    "save_strategy": "steps",
    "weight_decay": 0,
    "max_grad_norm": 0.3,
    "remove_unused_columns": false,
    "do_eval": false,
    "evaluation_strategy": "no"
}
```

**模型评估**

该部分待全量模型训练完毕后再进行补充

### TO-DO

- [ ]  更改Dataloader为动态加载文件方式，以适应超大规模数据
- [ ]  添加模型评估方法
- [ ]  添加交互界面以供测试，提供http调用

**FAQ**:

```python
1. 训练提示bitsandbytes报错？

 Transformer版本一定要用原库安装，一般安装命令

 pip install git+https:*//github.com/huggingface/transformers.git
 不过由于公司平台网络限制原因，建议直接下载git包后，通过pip install -e .
 的方式进行安装

2. 如果发生生成token重复怎么办？
 生成token重复通常可能因训练不充分，或数据集质量太差，另外也可通过生成参数去控制，具体可见[**此处**](https://yuansearch.com/2023/08/14/%E8%A7%A3%E7%A0%81%E7%9A%84%E7%94%9F%E6%88%90%E5%A4%9A%E6%A0%B7%E6%80%A7/)
```

以上为本周进展，本篇将根据具体实验进展不断更新，以上部分观点仅为个人见解，如有错误，望批评指正。