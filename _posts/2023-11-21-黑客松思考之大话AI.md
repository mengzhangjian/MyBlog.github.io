---
layout:     post
title:      黑客松思考之大话AI
subtitle:   是大模型呀
date:       2023-11-21
author:     BY    J
header-img: img/post-bg-mma-4.jpg
catalog: true
tags:
    - 大模型
---

# 黑客松思考之大话AI

因为种种原因，要离开啦。来不及一一道别。有收获也有遗憾，不管怎样，希望我们每个人都能找到最初的自己，祝各位安好。

因为之前参加了黑客松比赛，我们的作品ChatTravel拿到了最佳合作奖和最佳人气奖（虽然实现极其简单，望各位别吐槽啊 😂，但我们的美好愿景，还是希望滴滴可以成为集各出行能力于一体的 智能个人助手）。这次受主办方邀请，回顾下黑客松的参赛作品，恰逢组内分享，所以临走前发挥一下自己的余热吧。        AI是科技的未来，这毋庸置疑。尤其今年大模型与AIGC突然爆火，让很多人跃跃欲试参与进来，开发出自己理想中的作品，但是大模型是什么，为什么火起来，相信很多非学科同学不了解，不清楚，所以借这个契机，我写一下自己的浅见，做个普及讲解，望各位看官批评指正。

![https://res.cloudinary.com/dsn0i1fsm/image/upload/v1700571471/WechatIMG266_c70dy7.jpg](https://res.cloudinary.com/dsn0i1fsm/image/upload/v1700571471/WechatIMG266_c70dy7.jpg)

       

![https://res.cloudinary.com/dsn0i1fsm/image/upload/v1700571483/WechatIMG265_ffsmxe.jpg](https://res.cloudinary.com/dsn0i1fsm/image/upload/v1700571483/WechatIMG265_ffsmxe.jpg)

了解大模型，首先要了解AI。首先让我们先来了解下AI是什么，它目前处于什么阶段。[Google Cloud](https://cloud.google.com/learn/what-is-artificial-intelligence?hl=zh-cn)上有一个很好的定义:

```jsx
人工智能是一个构建能够推理、学习和行动的计算机和机器的科学领域，这种推理、学习和行动通常需要人类智力，或者涉及超出人类分析能力的数据规模。

AI 是一个广博的领域，涵盖许多不同的学科，包括计算机科学、数据分析和统计、硬件和软件工程、语言学、神经学，甚至哲学和心理学。

在业务使用的操作层面上，AI 是一组主要基于机器学习和深度学习的技术，用于数据分析、预测、对象分类、自然语言处理、推荐、智能数据检索等等。

人工智能的类型

人工智能可以采用多种方式进行组织，具体取决于开发阶段或正在执行的操作。
例如，AI 开发通常分为四个阶段。
1. 反应式机器：有限的 AI，仅根据预编程规则对不同类型的刺激做出反应。不使用内存，因此无法通过新数据进行学习。1997 年击败国际象棋冠军加里·卡斯帕罗夫的 IBM 深蓝超级计算机就是反应式机器的一个例子。
2. 有限内存：大多数现代 AI 都被视为具有有限内存的 AI。它可以通过使用新数据（通常是通过人工神经网络或其他训练模型）进行训练，从而使用内存随着时间的推移而改进。深度学习是机器学习的一部分，被视为具有有限内存的人工智能。
3. 心智理论：心智理论 AI 目前不存在，但研究正在实现其可能性。它描述了可以模拟人类思维并具有与人类相同的决策能力的 AI，包括识别和记忆情感以及在社交场合中像人类一样做出反应。 
4. 自我意识：自我意识 AI 比心智理论 AI 前进了一步，它描述了一种神秘的机器，这种机器知道自己的存在并具有人类的智力和情感能力。与心智理论 AI 一样，自我意识 AI 目前也不存在。

对人工智能类型进行广泛分类的一种更有用的方法是按照机器可以做什么来分类。我们目前所说的所有人工智能都被认为是“窄” (narrow) 人工智能，因为它只能根据其编程和训练来执行一组范围狭窄的操作。例如，用于对象分类的 AI 算法无法执行自然语言处理。Google 搜索是一种窄 AI，预测分析或虚拟助理也是窄 AI。

人工通用智能 (AGI) 是指机器可以像人类一样“感知、思考和行动”。AGI 目前不存在。下一个等级将是人工超级智能 (ASI)，即机器可以在所有方面发挥出优于人类的功能。
```

另附一张[Google DeepMind](https://arxiv.org/abs/2311.02462)出得各阶段AGI应具备的水平总结 

![https://res.cloudinary.com/dsn0i1fsm/image/upload/v1700571579/leve_of_agi_xgpr0k.png](https://res.cloudinary.com/dsn0i1fsm/image/upload/v1700571579/leve_of_agi_xgpr0k.png)

我们可以看到AI涵盖了非常多的学科，同时目前业务层面上，AI是基于机器学习和深度学习技术，用于执行各种功能的技术。而人类终极追求的AI是第三或者第四阶段，而大模型正因生成方式是人类交互友好型，且数据、算力、模型又发展到了一个新的高度（天花板级别），所以出现了大模型热潮，但根本逃脱不了拟合的范畴。这种方式最终是否会碰撞出其他火花，目前还不得而知，但至少让我们看到了AI面向通用智能的希望。最后我们可以总结当前的AI技术与**数据、机器学习、深度学习技术**高度相关。

      所以我们如果想了解当前的AI，只要对机器学习与深度学习技术有个基本的了解就清楚，也算是可以入门了。由于AI涵盖了多个学科，所以从每个学科角度都可以来讲解，因为我自身是自动化专业，所以下面从数值拟合的角度来为大家讲解一下AI发展史，专业性不对的地方望各位海涵。

      让我们从一元一次方程式开始吧！

$$
y = w *x + b
$$

如果我们已知$y, b$, 那么$x = (y - b) / w,  当w \neq 0$。当然，实际情况我们也可以求解$y 或者w$。它的函数图像为:

![https://res.cloudinary.com/dsn0i1fsm/image/upload/v1700571673/dimension_lu4m3o.png](https://res.cloudinary.com/dsn0i1fsm/image/upload/v1700571673/dimension_lu4m3o.png)

根据初中知识，我们可以知道红线的左侧为左半平面，右侧为右半平面。假设有两堆点（形状为分别为 🔺和 🟡），分别均匀的分布在左半平面与右半平面，那么我们可以说直线$y = x$可以完美的两堆点给分开，这样就完成了一个**分类问题**。对于红色直线上的点，当$x = 0 时， y = 0$, 那么当$x = 1,$ 根据方程，我们预测$y = 1$，那么这就是一个**回归预测问题。**至此机器学习中的分类与回归问题相信大家已经了解清楚了。

以二维平面为例，如果这两堆点不规则的分布在平面上，我们需要画一个曲线来将他们分开，下图左侧让我们直观感受下

![https://res.cloudinary.com/dsn0i1fsm/image/upload/v1700571719/hahah_wt5sho.png](https://res.cloudinary.com/dsn0i1fsm/image/upload/v1700571719/hahah_wt5sho.png)

可以看到，左图黑色曲线可以完美的将白点和黑点区分出来，但是似乎没有任何一种已知方程式可以画出该曲线。起初人们为了求解这类非线性问题，想了很多方法。对于曲线上的点，我们是否可以想在二维平面，每个点都是两条直线的交叉点，那么我可以求解出无数条直线的交叉点，那么我自然就得到了这条曲线。同时为了保证精准，我需要预测出曲线上的下一个点（我可以截取曲线中非常小的一段，知道这一点的斜率，便能知道下一点的精确位置，可以类比用汽车速度来预测汽车的位置）人们发明了最速下降法等各种方法来进行点的预测，这也是机器学习方式中梯度下降法的由来。小记：在数控机床中，切出圆等立方体，均是用无数直线拟合，来进行点的操作。

至此我们了解到了机器学习方法中的**分类与预测**问题，同时知道了**线性与非线性**的概念。

 但是，到这里，还是会有很多人问，我还是不知道机器学习是啥，神经网络又是什么东西，为什么有深度学习，难道机器学习不行吗？我们还是回归到

$$
y = w * x + b
$$

在二维世界中，这里$x$是一个点，设想在多维世界中，$x$就变成了$x1, x2, x3, ....$, 起初人们会用一些降维的方法，用基本的数学问题解决，但是当$x$的维度越来越高，问题呈现出更加非线性的特点，也就是说对于$x$是通过怎样的过程变化到$y$, 我们无法观察到显示的物理变化过程从而用数学方式建模出来，人们便想到了设定一个目标函数，例如我的目标是让$y - x$越来越接近，那么我如何通过自学习的方式利用已经拥有的大量数据，让机器自己学习出这个变换过程。人们不断开发出各种学习方法，而神经网络则被人们称为**深度学习**，而这些，我们可以笼统的成为**机器学习，**而这里通过设定目标函数，利用已知数据自学习的方式，则被称为**监督学习。**这里我们同样用方程式表达出来：

$$
Y = W * X + B
$$

这里我们用大写方式表达出来，以此来表示矩阵形态。我们通过图像的例子来形象的重复上述过程。例如$X$是一副1024 * 1024大小的人脸图片，我们将其展开为一个列向量则是 $1024 * 1024 * 1 = 1048576 * 1$,  通常图像又包含R，G，B三个维度，则 最终形态为$(1048576 * 3) * 1$。 我们发现，天啊，这么大的向量，计算机怎么处理的了呢？这就是通常人们说的**维度灾难。**在算力未获得突破前（对，就是你想的那样，NVIDIA显卡未应用到计算上前），人们也是想了各种降维方法，PCA主成分分析，积分图降低计算量，稀疏求解等等各种方法，来解决维度灾难问题，从而让$X$的维度变得可控，这也是我们通常所说的**特征提取过程的由来。**

还是回到我们的人脸图片，我们打开图片，突然发现我的人脸，竟然被人打了马赛克，糟糕，还给我剪成了52 * 52的分辨率，是哪个坏蛋，还有没有王法了。由于这些变换过程我们无法用数学方式建模出来，即

![https://res.cloudinary.com/dsn0i1fsm/image/upload/v1700571764/face_l6bajv.png](https://res.cloudinary.com/dsn0i1fsm/image/upload/v1700571764/face_l6bajv.png)

$$
Y = W1 * W2 * W3 ....* X + B
$$

这里 $W_{1- n}$分别代表了马赛克，插值等各种操作，起初人们应用各种矩阵求解方法来求解这个变换过程，通常我们说这个方程有解析解，非解析，该问题是凸优与非凸优问题，也是从这里出来的，下次碰到别人说该问题是个非凸问题，你也可以插一嘴了 🤪。

后来神经网络遇上了算力爆发，这个万金油百试不爽，人们不用再费时费力去想各种矩阵求解问题，不用再去假设各种变换过程，网络堆叠简单，同时效果又出其的好。神经网络自此一发不可收拾。

说了这么多，总之，对于各种问题，由于变换过程的复杂性，机器学习就是在解决如何更好的拟合$X 到Y$的变换过程。

到了这里，你是不是想问，我常听别人说CNN，LSTM，Transformer，你是压根没提啊。我还是不懂，这些到底是什么鬼东西。

前面我们用图像假设为$X$，但是世界上的信号源还有我们熟知的文本，语音，传感器信号，雷达波，商品特征等等各种。仅以图像，文本而言，由于他们的形态差异较大，一般而言图像（RGB图像）在计算机中的表达是这样的

![https://res.cloudinary.com/dsn0i1fsm/image/upload/v1700571811/rgb_kmqbr3.png](https://res.cloudinary.com/dsn0i1fsm/image/upload/v1700571811/rgb_kmqbr3.png)

而文本是一串字符串或一段话，例如我爱北京天安门，假设我们的字典大小为100，而我爱北京天安门在字典处于前7的位置，那在计算机中表达最简单的方式可以是$[1, 2,3,4,5,6,7,0,0,.......,0]$, 总长度为100。这就是我们常说的token。我们可以直观的看到图像更加注重空间特征，而文本则更加注重时序特征。所以如何处理$X$使之更好的去理解我们的数据，同时可以理解为更好的提取特征，人们则设计了不同的网络用于$X$的处理，其中CNN用于处理图像，通过滑动卷积以提取空间特征。而LSTM与Transformer则用于处理文本或者传感器这种时序数据，值得注意的是近几年Transformer的自注意力机制，本来用于处理文本中单个序列内部间的相似关系，但是由于图像本身存在天然的非局部自相似性，所以现在Transformer应用到图像中也取得了非常好的效果。

从信号的领域来讲，由于我们可以笼统的将各种信息，例如图像，文本等，称之为信号，所以从小波变换的角度来讲，我们可以将图像信号分解为各种不同频率的正弦波（或者泰勒级数展开的正余弦形式），所以在Transformer中输入X的位置编码，作者采用正余弦进行编码，可以使模型更好的处理序列数据，或许从这个角度看，这样更加具备了可解释性。

总之，机器学习或者深度学习就是在学习理解$X$ 与$Y$ 之间的关系，如何处理$X$ , 如何更好的学习，其中涉及了数据收集与处理，方法设计等。这需要算法同学具备一定的理论知识，很好的业务理解能力，同时还需要具备较好的工程素养。我们在理解算法同学的模型时，只要理清了业务逻辑，确认好输入$X$ 与输出$Y$,自然能清楚理解算法是如何做的了。

说到这里，让我们回看GPT是什么，为什么火起来，首先GPT全称(Generative Pre-trained Transformer)，可以看到其基础模型依然是Transformer，只不过堆叠了Transformer的解码-即decoder部分，由于是按字预测，所以是Generative模型。那它为什么火起来呢？个人理解，我们人类表达时也是对话方式，而生成模型的数据输出方式本身即是人类友好型，由于GPT的算力，创新的三阶段训练方式，使数据拟合达到了一个新的高度，同时数据输出的友好性，使人们看到了通用AI发展的新希望，所以它火了。至于这种方式，会不会迸发出通用AI的能力，使其具备决策，逻辑推理的能力，无数的AI开发者们正在继续探索。

最后，大家对于AI的发展是不是有了清晰的认识了呢，还是越看越糊涂了 😂,能力有限，有什么不对的地方，且看且评吧，至于为什么讲，是因为在接触工程同学或者部门实习生时，望AI而却步，或者只知神经网络，所以啰嗦这么多，可以使各业务线同学对于目前的AI发展有个基本的认识，可以使我们更好的理解AI算法，拥抱它，投入它，发展它，为科技树的绽放贡献自己的一份力量。

而黑客松各个参赛作品，正是表现出了大家对于通用AI的热情，从业务，兴趣爱好角度，为科技树绽放贡献自己能力的结果。这块没啥好说的，改一改李厂长的一句话，每一个产品都值得用通用AI技术重做一遍，大模型可能是一个新的起点。

再次感谢主办方举办如此有意义的比赛，这也是我在滴滴最开心的时刻。