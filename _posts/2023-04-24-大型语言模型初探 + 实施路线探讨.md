# 大型语言模型初探 + 实施路线探讨

​		4月初惊艳于ChatGPT与stable diffusion的模型效果，才发觉自己跟业界最新进展落后的程度很大(还停留在Bert时代)，所以对相关内容开始了一顿恶补，并初步进行了动手尝试，下面简要总结下大模型前期的调研与这两天的实践成果。

​		近期做的尝试主要进行了模型微调与上层应用构建两件事，时间跨度分别为10天与1天。起初模型微调效果不理想，中旬就有点沮丧便放弃了当初的计划(打造个人助理)，最近OpenAI上层应用层出不穷，偶然间发现了chatpdf.com，作为打工人，当然要从业务角度出发，更多层面是因为之前组里做过简历解析与Ebot的业务，所以惊叹于大模型在上述两个业务的效果，由此省下的成本，担忧算法调参师慢慢就被市场淘汰，同时也对算法程序员提出了更高的要求。通过对chatpdf的原理分析，进行了简单实现。下面是结合了Ebot知识库(2000多条问答)与清华Chatglm-6B的简单实现:

​       http://10.191.148.106:8098/

可以尝试提问以下问题:

* 请问商旅的出差制度
* 如何登陆管理员后台
* 如何下载滴滴APP

​       言归正传，下面将主要跟大家分享下截止4月中旬，汇总的社区开源项目以及自己在模型微调、大模型与业务结合等方面的思考。

​       每当深度学习算法相关的领域取得突破进展时，必然伴随着 数据、模型(训练+微调)、上层应用(部署交互)三方面的紧密配合，同时大量的创业公司涌现也是围绕这三方面展开。数据方面: ChatGPT的技术报告是组织排序对并以反馈方式展开模型的奖励训练，目前社区组织的高质训练数据基于以调用OpenAI接口产出，所以再无须构造排序对，由于部分开源数据跟项目混合存在，所以会在备注进行添加说明。

​		

|                           开源项目                           | 介绍                                  | 备注                                                 |
| :----------------------------------------------------------: | :------------------------------------ | :--------------------------------------------------- |
|             https://github.com/THUDM/ChatGLM-6B              | 清华开源双语对话语言模型              | 1.无开源数据<br />2.实测中文效果最好<br />           |
|             https://github.com/LianjiaTech/BELLE             | 链家开源中文对话大模型                | 1.未实测，效果未知<br />2.大量高质中文数据（很实在） |
| https://github.com/tloen/alpaca-lora<br />https://github.com/LC1332/Chinese-alpaca-lora<br />https://github.com/masa3141/japanese-alpaca-lora<br />https://github.com/22-hours/cabrita | 基于LLaMA语言模型的<br />羊驼项目汇总 | 1.可基于链家开源数据<br />直接一键训练               |
|             https://github.com/karpathy/nanoGPT              | 用于快速训练类GPT模型的训练库         |                                                      |
|          https://github.com/Lightning-AI/lit-llama           | 基于nanoGPT实现，Apache2.0 license    |                                                      |
|              https://github.com/lm-sys/FastChat              | Vicuna                                | 1.记得实测效果很惊艳，生成速度很快                   |
|        https://github.com/InsaneLife/ChineseNLPCorpus        | 做实验用的NLP数据预料集               |                                                      |
|       https://github.com/brightmart/nlp_chinese_corpus       | ChatYuan的作者整理的语料              |                                                      |
|       https://github.com/binary-husky/chatgpt_academic       | 大模型上层应用                        |                                                      |
|    https://gpt-index.readthedocs.io/en/latest/index.html     | 大模型上层应用                        |                                                      |
|       https://github.com/Significant-Gravitas/Auto-GPT       | 大模型上层应用                        | 上层应用太多，列举不过来                             |

## 模型训练

​		基于公司鲁班平台的算力及大模型的参数选择、数据来源、数据量、训练时间成本综合衡量，因为训练效果不可知且周期较长，所以当初选择了易于上手的alpaca-lora进行微调。

​       由于全参数微调成本巨大，目前社区主流微调方式有lora, P-tuing,  Adapter等方式。基于https://github.com/masa3141/japanese-alpaca-lora项目加链家开源数据，本人爬虫WikiHow数据, 具体训练参数如下：

* 单卡A100
* BATCH_SIZE 768
* MICOR_BATCH_SIZE 16
* EPOCH 3
* 训练数据: 200多万指令对

​      整个微调过程持续了10多天，迭代效果感官上还是可以的，但是针对WikiHow的爬虫数据微调效果不理想，复盘发现，WikiHow的回答数据太大，这导致了模型训练回答出现截断的原因。

​     由于上述过程持续时间过长加之效果不理想，另外大模型经常产出非事实性回答，部署成本巨大严重打消了我的积极性，所以对该过程的探索搁置了一星期，直到chatpdf进入我的视野。

## 上层应用探索

​		从公司业务角度出发，在当前算力下，大模型微调成本巨大， 单从这个角度就限制了一般公司在该方面的探索应用。但是我们又看到目前关于大模型的各上层应用层出不穷，所以如何结合开源大模型与公司现有业务是我主要思考的问题。

​        chatpdf这个应用主要是通过解析PDF，运用OpenAI可以与用户针对PDF进行非常好的交互式回答，体验效果非常好。其主要原理是将PDF分片向量化构建索引，通过对用户的问答进行文档检索，从而为大模型构建回答知识库，解决了token的限制与大模型非事实性回答的问题，从而带来了极其友好的交互体验。

​       从这一角度出发，我们可以针对任一知识库或是搜索引擎结果，来构建自己的个人助手。基于这一想法，结合企业级的问答知识库，我快速构建了这一简单应用。

​	   实现较为简单，主要利用faiss为知识库构建向量索引，语义模型提取向量，通过召回结果来构建chatglm6B的prompt，通过实验发现如何更好的构造prompt，会极大的影响模型回答结果，这也验证了度厂老板说的那句话。目前对于一些回答结果，模型仍会自组织产出与原召回不一致的回答。

## 未来探索

1. 微调自己的模型这条路最终要走，也非走不可
2. 尝试添加搜索引擎，研究auto-gpt的方式，使其更加智能，也期待清华尽早公布chatglm13B
3. 目前有很多惊艳的voice-clone开源项目，后续接入voice，打造智能数字人

## 思考

1. 小模型喂大数据导致模型过拟合的话，由于数据逐年递增，这是否意味着以后的模型会越来越大？
2. 目前情况下，对于QA类问答，大模型只是起到了基于知识库的排序输出的作用，是否大材小用？以小模型进行该方式训练是否就能起到很好的效果
3. NLP同计算机视觉一样，交互方式上变得越来越简易且交互友好，使得一般大众都能简易上手，可以快速构建自己的应用，对于算法程序员来说，挑战越来越大，必须不断学习，提高自己的门槛。（打工人不容易啊！）

