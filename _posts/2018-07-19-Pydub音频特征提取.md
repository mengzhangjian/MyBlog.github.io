---
layout:     post
title:      频谱处理可视化教程
subtitle:   WWDC 2018 Keynote 全记录
date:       2018-07-19
author:     BY
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - audio
---


# 音频-输入特征的可视化
## 波形与频谱图
## MFCC
## 三维频谱图
## Silence 移除
## 重采样-维度减少
## 特征抽取步骤
所需要的模块
```
import os
from os.path import isdir,join
from pathlib import Path
import pandas as pd
#Math
import numpy as np
from scipy.fftpack import fft
from scipy import signal
from scipy.io import wavfile
import librosa
from sklearn.decomposition import PCA

#Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import IPython.display as ipd
import librosa.display

import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go
import plotly.tools as tls
import pandas as pd

%matplotlib inline
```
```
Note:python2.7 import librosa
#报错,TypeError: expected string or buffer
#需要
pip uninstall joblib
pip install joblib==0.11
```
# 可视化
关于人类听力有两种理论-[基于频率](https://en.wikipedia.org/wiki/Place_theory_(hearing))与[时间理论](https://en.wikipedia.org/wiki/Temporal_theory_(hearing))在语音识别中，有两种倾向-收入频谱(频率)以及更加复杂的特征- MFCC-(Mel-Frequency Cepstral Coefficients)。通常我们很少直接操作原始数据。
# 波形与频谱图
```
trian_audio_path = ''
filename = ''
sample_rate,samples = wavfile.read(str(train_audio_path) + filename)
```
定义一个函数用来计算频谱
说明，我们将采取频谱值的对数形式。我们需要确保输入没有0值。
```
def  log specgram(audio,sample_rate,window_size=20,step_size=10,eps=1e-10):
    nperseg = int(round(window_size * sample_rate / 1e3))
    noverlap = int(round(step_size * sample_rate/ 1e3))
    freqs,times,spec = signal.spectrogram(audio,fs=sample_rate,window = 'hann',nperseg = nperseg,noverlap = noverlap,detrend = False)
    return freps,times,np.log(spec.T.astype(np.float32) + eps)
```

```
freqs,times,spectrogram samples,sample_rate = librosa.load(str(train_audio_path)+filename)= log_specgram(samples,sample_rate)
fig = plt.figure(figsize = (14,8))
axl = fig.add_subplot(211)
axl.set_title('Raw wave of ' + filename)
axl.set_ylabel('Amplitude')
axl.plot(np.linspace(0,sample_rate/len(samples),sample_rate),samples)
ax2 = fig.add_subplot(212)
ax2.imshow(spectrogram.T,aspect = 'auto',origin = 'lower',extent=[times.min(),times.max(),freqs.min(),freqs.max()])
ax2.set_yticks(freqs[::16])
ax2.set_xticks(times[::16])
ax2.set_title('Spectrogram of ' + filename)
ax2.set_ylabel('Freqs in Hz')
ax2.set_xlabel('Seconds')

```
上述针对自己的音频文件，可能会报错 x and y must have same dimension
修改
```

sample_rate, samples = wavfile.read(str(train_audio_path)+filename)
#samples,sample_rate = librosa.load(str(train_audio_path)+filename)
if samples.shape !=16000:
        samples = samples[:16000]
```
![显示](http://pbe1y6vc7.bkt.clouddn.com/1.png)

如果我们将频谱作为输入特征去进行识别，我们一定要记得均一化特征。(我们需要在所有的数据集上均一化，下面是例子)
```
mean = np.mean(spectrogram, axis=0)
std = np.std(spectrogram, axis=0)
spectrogram = (spectrogram - mean) / std

```
需要指出的是，对于每一帧，我们有160个特征，频率在0到8000。这意味着，一个特征对应着50HZ。然而，在1000-2000的频率倍程范围内，耳朵的频率分辨率为3.6HZ。这意味着，人类所听到的远远要比上述频谱要表达的所精确。
##1.2 [MFCC](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)(梅尔频率倒谱系数)
任何自动语音识别系统的第一步是提取特征，即识别音频信号的组成部分，这些组成部分有助于识别语言内容并丢弃所有其他携带诸如背景噪声，情绪等信息的东西。

理解语音的要点是人类产生的声音被声道的形状过滤，包括舌头，牙齿等。这种形状决定了声音的来源。如果我们能够准确地确定形状，这应该能够准确地表示正在生产的音素。声道的形状表现在短时功率谱的包络中，MFCC的工作是准确地表示这个包络。本页面将提供有关MFCC的简短教程。

Mel频率倒谱系数（MFCC）是一种广泛用于自动语音和说话人识别的功能。它们是戴维斯和梅尔斯坦在20世纪80年代引入的，从那以后一直是最先进的。在引入MFCC之前，线性预测系数（LPC）和线性预测倒谱系数（LPCC）（[点击此处获取有关倒谱和LPCC的教程](http://www.practicalcryptography.com/miscellaneous/machine-learning/tutorial-cepstrum-and-lpccs/)）并且是自动语音识别（ASR）的主要特征类型，特别是对于HMM分类器。本页将介绍MFCC的主要方面，为什么它们为ASR提供了一个很好的功能，以及如何实现它们。

```
可以用librosa python 包计算梅尔功率频谱和MFCC
#From this tutorial
S = librosa.feature.melspectrogram(samples,sr = sample_rate,n_mels=128)
#Convert to log scale(dB).We'll use the peak power(max) as reference
log_S = librosa.power_to_db(S, ref=np.max)

plt.figure(figsize=(12, 4))
librosa.display.specshow(log_S, sr=sample_rate, x_axis='time', y_axis='mel')
plt.title('Mel power spectrogram ')
plt.colorbar(format='%+02.0f dB')
plt.tight_layout()
```
可能会报错：ParameterError: data must be floating-point
这是因为wavfile.read()
```
samples,sample_rate = librosa.load(str(train_audio_path)+filename)
S = librosa.feature.melspectrogram(samples,sr = sample_rate,n_mels=128)
```
![显示](http://pbe1y6vc7.bkt.clouddn.com/3.png)

```
mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=13)

# Let's pad on the first and second deltas while we're at it
delta2_mfcc = librosa.feature.delta(mfcc, order=2)

plt.figure(figsize=(12, 4))
librosa.display.specshow(delta2_mfcc)
plt.ylabel('MFCC coeffs')
plt.xlabel('Time')
plt.title('MFCC')
plt.colorbar()
plt.tight_layout()
```
![显示](http://pbe1y6vc7.bkt.clouddn.com/4.png)
在经典乃至最先进的系统中，MFCC或者类似特征均被采用为系统的输入而不是频谱。
然而，在端到端系统中(基于神经网络的系统)大部分的输入特征可能是原始频谱或者是梅尔功率频谱。例如，MFCC去相关特征，但是NNS系统可以很好的处理相关特征。所以，如果你理解mel滤波器，你可能认为他们的用法合理。
怎样选择由你决定！
## 三维频谱
## Silence removal
```
ipd.Audio(samples,rate = sample_rate)
```
我一直认为一些VAD(声音活动检测)在这里是真正有用的。虽然单词很简短，但是在这有很多的silence.一个合适的VAD可以很大程度上减少训练大小，加速训练速度。让我们掐头去尾，然后再听一遍。(基于上图绘制，截取4000到13000)
截取后，我们依然能听到整个单词。我们不可能手动基于简单绘图裁剪所有的文件。但是你可以用webrtcvad包去做一个好的VAD。
我们再次绘制，并进行字母对齐。
```
freqs, times, spectrogram_cut = log_specgram(samples_cut, sample_rate)

fig = plt.figure(figsize=(14, 8))
ax1 = fig.add_subplot(211)
ax1.set_title('Raw wave of ' + filename)
ax1.set_ylabel('Amplitude')
ax1.plot(samples_cut)

```























